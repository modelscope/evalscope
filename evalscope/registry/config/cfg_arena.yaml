# input raw data
question_file: registry/data/question.jsonl

# candidate models to be battled
answers_gen:
    chatglm3-6b:
        # model_id_or_path could be local absolute path, e.g. /to/path/.cache/modelscope/ZhipuAI/chatglm3-6b
        model_id_or_path: ZhipuAI/chatglm3-6b       # model_id on modelscope
        revision: v1.0.2        # revision of model, default is NULL
        precision: torch.float16
        enable: true            # enable or disable this model
        template_type: chatglm3  # see: https://github.com/modelscope/swift/blob/main/docs/source/LLM/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.md
        generation_config:
            do_sample: true
            max_new_tokens: 256
            top_k: 20
            top_p: 0.75
            temperature: 0.333
        # output predicted answer file name
        output_file: registry/data/arena/answers/answer_chatglm3-6b.jsonl
    Baichuan2-7B-Base:
        model_id_or_path: baichuan-inc/Baichuan2-7B-Base
        revision: v1.0.2       # revision of model, default is NULL
        precision: torch.float16
        enable: false           # enable or disable this model
        template_type: default-generation
        generation_config:
            do_sample: true
            max_new_tokens: 256
            top_k: 20
            top_p: 0.75
            temperature: 0.3
        output_file: registry/data/arena/answers/answer_Baichuan2-7B-Base.jsonl
    Qwen-7B:
        model_id_or_path: qwen/Qwen-7B
        revision: v1.1.8       # revision of model, default is NULL
        precision: torch.float16
        enable: true           # enable or disable this model      # TODO: tokenizer issue
        template_type: default-generation
        generation_config:
            do_sample: true
            max_new_tokens: 256
            top_k: 20
            top_p: 0.75
            temperature: 0.3
        output_file: registry/data/arena/answers/answer_Qwen-7B.jsonl

# Auto-reviewer(GPT-4) config
reviews_gen:
    enable: true
    reviewer:
        # class reference of auto reviewer(GPT-4)
        ref: evalscope.evaluator.reviewer.auto_reviewer:AutoReviewerGpt4
        args:
            max_tokens: 1024
            temperature: 0.2
            # options: pairwise, pairwise_baseline, single (default is pairwise)
            mode: pairwise
            # position bias mitigation strategy, options: swap_position, randomize_order, NULL. default is NULL
            position_bias_mitigation: NULL
            # completion parser config, default is lmsys_parser
            fn_completion_parser: lmsys_parser
    # prompt templates for auto reviewer(GPT-4)
    prompt_file: registry/data/prompt_template/prompt_templates.jsonl
    # target answer files list to be reviewed,
    # could be replaced by your own path: ['/path/to/answers_model_1.jsonl', '/path/to/answers_model_2.jsonl', ...]
    # Default is NULL, which means all answers in answers_gen will be reviewed
    target_answers: NULL
    # output file name of auto reviewer
    review_file: registry/data/arena/reviews/review_gpt4.jsonl

# rating results
rating_gen:
    enable: true
    metrics: ['elo']
    # elo rating report file name
    report_file: registry/data/arena/reports/elo_rating_origin.csv
