from pydantic import BaseModel, Field
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union

from evalscope.utils.logger import get_logger

if TYPE_CHECKING:
    from .state import TaskState

logger = get_logger(name=__name__)

Value = Dict[str, Union[int, float, bool]]


class Score(BaseModel):
    """Score generated by a scorer."""

    value: Value = Field(default_factory=dict)
    """Score value as a dictionary. Key is the score name, value is the score value."""

    answer: Optional[str] = Field(default=None)
    """Answer extracted from model output (optional)"""

    prediction: Optional[str] = Field(default=None)
    """Original prediction text from the model (optional)"""

    explanation: Optional[str] = Field(default=None)
    """Explanation of score (optional)."""

    metadata: Optional[Dict[str, Any]] = Field(default=None)
    """Additional metadata related to the score"""

    main_score: Optional[str] = Field(default=None)
    """Main score name, if applicable. This is used to indicate which score is the primary score in a multi-score scenario."""  # noqa: E501

    @property
    def main_value(self) -> Union[int, float, bool]:
        """Main score value."""
        if self.main_score and self.main_score in self.value:
            return self.value[self.main_score]
        return next(iter(self.value.values()), None)


class SampleScore(BaseModel):
    """Score for a Sample."""

    score: Score
    """A score"""

    sample_id: Optional[Union[str, int]] = Field(default=None)
    """A sample id"""

    sample_metadata: Optional[Dict[str, Any]] = Field(default=None)
    """Metadata from the sample"""

    scorer: Optional[str] = Field(default=None)
    """Registry name of scorer that created this score."""


class Scorer:

    def __call__(
        self,
        state: 'TaskState',
    ) -> SampleScore:
        r"""Score model outputs.

        Evaluate the passed outputs and targets and return a
        dictionary with scoring outcomes and context.

        Args:
            state: Task state
        Returns:
            Score: Scoring outcome
        """
        ...


class AggScore(BaseModel):
    """Output of an aggregation operation."""

    score: float = Field(default=0.0)
    """Aggregated value as a float."""

    metric_name: str = Field(default='')
    """Name of the metric being aggregated."""

    num: int = Field(default=0)
    """Number of samples used in the aggregation."""

    ids: Optional[List[Union[str, int]]] = Field(default=None)
    """List of sample IDs used in the aggregation, if applicable."""

    metadata: Optional[Dict[str, Any]] = Field(default=None)
    """Additional metadata related to the aggregation."""


class Aggregator:

    def __call__(self, scores: List[SampleScore]) -> List[AggScore]:
        r"""Aggregate a metric on a list of scores.

        Args:
          scores: List of scores.

        Returns:
          List[AggregatOutput]: List of aggregated outputs.
        """
        ...
