import gradio as gr
import os
from typing import AsyncGenerator, Tuple
from utils import convert_eval_args_to_config, convert_perf_args_to_config, submit_and_poll

DEFAULT_SERVICE_URL = os.getenv('EVALSCOPE_SERVICE_URL', 'http://127.0.0.1:9000')
VALID_EVAL_BENCHMARKS = ['gsm8k', 'mmlu', 'cmmlu', 'ceval', 'arc', 'math_500', 'aime24', 'aime25']


def create_eval_interface(service_url_input, poll_interval_input, common_model_name, common_api_url, common_api_key):
    """Create the content for the evaluation task interface"""
    with gr.Row():
        # --- Left Column: Configuration (Scale 2) ---
        with gr.Column(scale=2, variant='panel'):
            gr.Markdown('### ğŸ› ï¸ è¯„ä¼°é…ç½®')

            with gr.Accordion('è¯„ä¼°è®¾ç½®', open=True):
                eval_datasets = gr.Dropdown(
                    label='æ•°æ®é›†',
                    choices=VALID_EVAL_BENCHMARKS,
                    value=['gsm8k'],
                    multiselect=True,
                    allow_custom_value=True,
                    info='é€‰æ‹©æˆ–è¾“å…¥ç”¨äºè¯„ä¼°çš„æ•°æ®é›†åç§°ï¼Œæ”¯æŒå¤šé€‰æˆ–è‡ªå®šä¹‰è¾“å…¥ï¼ˆé€—å·åˆ†éš”ï¼‰'
                )
                eval_limit = gr.Number(label='é™åˆ¶æ•°é‡', value=5, precision=0, info='æ¯ä¸ªæ•°æ®é›†çš„ä»»åŠ¡æ•°é‡é™åˆ¶ï¼Œ-1è¡¨ç¤ºä¸é™åˆ¶')
                eval_batch_size = gr.Number(label='æ‰¹å¤§å°', value=1, precision=0, info='æ¯æ¬¡æ¨¡å‹è¯·æ±‚çš„æ‰¹å¤„ç†å¤§å°')

            with gr.Accordion('æ›´å¤šå‚æ•°', open=False):
                eval_repeats = gr.Number(label='é‡å¤æ¬¡æ•°', value=1, precision=0, info='é‡å¤è¿è¡Œè¯„ä¼°çš„æ¬¡æ•°')
                eval_timeout = gr.Number(label='è¶…æ—¶æ—¶é—´ (ç§’)', value=3600, info='ä»»åŠ¡è¿è¡Œçš„æœ€å¤§è¶…æ—¶æ—¶é—´')
                eval_stream = gr.Checkbox(label='æµå¼è¾“å‡º', value=True, info='æ˜¯å¦ä»¥æµå¼æ–¹å¼è·å–æ¨¡å‹å“åº”')

                gr.Markdown('#### æ¨¡å‹ç”Ÿæˆå‚æ•°')
                eval_temp = gr.Slider(
                    label='Temperature (éšæœºæ€§)', minimum=0.0, maximum=1.0, value=0.0, info='ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šå¤§è¶Šéšæœº'
                )
                eval_top_p = gr.Slider(
                    label='Top P (æ ¸é‡‡æ ·)', minimum=0.0, maximum=1.0, value=1.0, info='æ ¸é‡‡æ ·å‚æ•°ï¼Œåªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°Pçš„è¯'
                )
                eval_max_tokens = gr.Number(label='Max Tokens (æœ€å¤§ç”Ÿæˆ)', value=1024, precision=0, info='æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦')
                eval_top_k = gr.Number(label='Top K (Top K é‡‡æ ·)', value=50, precision=0, info='Top K é‡‡æ ·å‚æ•°ï¼Œåªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„Kä¸ªè¯')

                dataset_args = gr.Code(label='æ•°æ®é›†å‚æ•° (JSON)', language='json', value='{}', lines=2, max_lines=10)

            btn_eval = gr.Button('ğŸš€ å¼€å§‹è¯„ä¼°', variant='primary', size='lg')

        # --- Right Column: Logs and Progress (Scale 3) ---
        with gr.Column(scale=3):
            gr.Markdown('### è¿è¡ŒçŠ¶æ€ä¸æ—¥å¿—')
            eval_progress_status = gr.Markdown('å½“å‰çŠ¶æ€: å‡†å¤‡å°±ç»ª', label='è¯„ä¼°ä»»åŠ¡çŠ¶æ€')
            eval_logs = gr.Code(
                label='æ§åˆ¶å°è¾“å‡º', language='shell', interactive=False, lines=30, elem_classes=['log-panel'], max_lines=30
            )

    # Logic handling function
    async def run_eval_wrapper(
        service_url,
        interval,
        model,  # Get from common model name
        api_url,  # Get from common API URL
        api_key,  # Get from common API Key
        datasets,
        limit,
        batch_size,
        repeats,
        timeout,
        stream,
        temp,
        top_p,
        max_tokens,
        top_k,
        ds_args,
    ) -> AsyncGenerator[Tuple[str, str], None]:
        payload = convert_eval_args_to_config(
            model=model,
            api_url=api_url,
            api_key=api_key,
            datasets=datasets,
            limit=limit,
            eval_batch_size=batch_size,
            repeats=repeats,
            timeout=timeout,
            stream=stream,
            temperature=temp,
            top_p=top_p,
            max_tokens=max_tokens,
            top_k=top_k,
            dataset_args=ds_args
        )
        async for log_content, progress_status_text in submit_and_poll(service_url, 'eval', payload, interval):
            yield log_content, progress_status_text

    btn_eval.click(
        run_eval_wrapper,
        inputs=[
            service_url_input, poll_interval_input, common_model_name, common_api_url, common_api_key, eval_datasets,
            eval_limit, eval_batch_size, eval_repeats, eval_timeout, eval_stream, eval_temp, eval_top_p,
            eval_max_tokens, eval_top_k, dataset_args
        ],
        outputs=[eval_logs, eval_progress_status]
    )


def create_perf_interface(service_url_input, poll_interval_input, common_model_name, common_api_url, common_api_key):
    """Create the content for the performance testing interface"""
    with gr.Row():
        # --- Left Column: Configuration (Scale 2) ---
        with gr.Column(scale=2, variant='panel'):
            gr.Markdown('### âš¡ æ€§èƒ½æµ‹è¯•é…ç½®')

            with gr.Accordion('å‹æµ‹è®¾ç½®', open=True):
                perf_api_type = gr.Dropdown(
                    label='APIç±»å‹', choices=['openai'], value='openai', info='æŒ‡å®šAPIæ¥å£ç±»å‹ï¼Œç›®å‰æ”¯æŒOpenAIå…¼å®¹æ¥å£'
                )
                perf_parallel = gr.Textbox(
                    label='å¹¶å‘æ•° (é€—å·åˆ†éš”)', value='1', placeholder='ä¾‹å¦‚: 1,2,4', info='é€—å·åˆ†éš”çš„å¹¶å‘ç”¨æˆ·æ•°åˆ—è¡¨ï¼Œå¯å®šä¹‰å¤šä¸ªå¹¶å‘ç­‰çº§'
                )
                perf_number = gr.Textbox(
                    label='æ€»è¯·æ±‚æ•° (é€—å·åˆ†éš”)', value='10', placeholder='ä¾‹å¦‚: 10,20', info='é€—å·åˆ†éš”çš„æ€»è¯·æ±‚æ•°åˆ—è¡¨ï¼Œå¯¹åº”æ¯ä¸ªå¹¶å‘ç­‰çº§çš„æ€»è¯·æ±‚æ•°'
                )
                perf_rate = gr.Number(label='é€Ÿç‡é™åˆ¶ (è¯·æ±‚/ç§’)', value=-1, precision=0, info='-1 è¡¨ç¤ºä¸é™åˆ¶æ¯ç§’è¯·æ±‚æ•°')

            with gr.Accordion('æ¨¡å‹ç”Ÿæˆå‚æ•°', open=False):
                perf_max_tokens = gr.Number(label='Max Tokens (æœ€å¤§ç”Ÿæˆ)', value=2048, precision=0, info='æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦')
                perf_min_tokens = gr.Number(label='Min Tokens (æœ€å°ç”Ÿæˆ)', value=0, precision=0, info='æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€å°é•¿åº¦')

                perf_temp = gr.Slider(
                    label='Temperature (éšæœºæ€§)', minimum=0.0, maximum=1.0, value=0.0, info='ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šå¤§è¶Šéšæœº'
                )
                perf_top_p = gr.Slider(
                    label='Top P (æ ¸é‡‡æ ·)', minimum=0.0, maximum=1.0, value=1.0, info='æ ¸é‡‡æ ·å‚æ•°ï¼Œåªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°Pçš„è¯'
                )

            with gr.Accordion('æ•°æ®é›†è®¾ç½®', open=False):
                perf_dataset = gr.Dropdown(
                    label='æµ‹è¯•æ•°æ®é›†', choices=['openqa', 'line_by_line', 'random'], value='openqa', info='ç”¨äºæ€§èƒ½æµ‹è¯•çš„æ•°æ®é›†ç±»å‹'
                )

                perf_max_prompt = gr.Number(
                    label='Max Prompt Len (æœ€å¤§Prompté•¿åº¦)', value=1024, precision=0, info='ç”ŸæˆPromptçš„æœ€å¤§é•¿åº¦'
                )
                perf_min_prompt = gr.Number(
                    label='Min Prompt Len (æœ€å°Prompté•¿åº¦)', value=0, precision=0, info='ç”ŸæˆPromptçš„æœ€å°é•¿åº¦'
                )

            btn_perf = gr.Button('âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•', variant='primary', size='lg')

        # --- Right Column: Logs and Progress (Scale 3) ---
        with gr.Column(scale=3):
            gr.Markdown('### è¿è¡ŒçŠ¶æ€ä¸æ—¥å¿—')
            perf_progress_status = gr.Markdown('å½“å‰çŠ¶æ€: å‡†å¤‡å°±ç»ª', label='æ€§èƒ½æµ‹è¯•ä»»åŠ¡çŠ¶æ€')
            perf_logs = gr.Code(
                label='æ§åˆ¶å°è¾“å‡º', language='shell', interactive=False, lines=30, elem_classes=['log-panel'], max_lines=30
            )

    # Logic handling function
    async def run_perf_wrapper(
        service_url,
        interval,
        model,  # Get from common model name
        url,  # Get from common API URL
        api_key,  # Get from common API Key
        api_type,
        parallel,
        number,
        rate,
        max_tokens,
        min_tokens,
        temp,
        top_p,
        dataset,
        max_pl,
        min_pl,
    ) -> AsyncGenerator[Tuple[str, str], None]:
        payload = convert_perf_args_to_config(
            model=model,
            url=url,
            api=api_type,
            api_key=api_key,
            parallel=parallel,
            number=number,
            rate=rate,
            max_tokens=max_tokens,
            min_tokens=min_tokens,
            temperature=temp,
            top_p=top_p,
            dataset=dataset,
            max_prompt_length=max_pl,
            min_prompt_length=min_pl
        )
        async for log_content, progress_status_text in submit_and_poll(service_url, 'perf', payload, interval):
            yield log_content, progress_status_text

    btn_perf.click(
        run_perf_wrapper,
        inputs=[
            service_url_input, poll_interval_input, common_model_name, common_api_url, common_api_key, perf_api_type,
            perf_parallel, perf_number, perf_rate, perf_max_tokens, perf_min_tokens, perf_temp, perf_top_p,
            perf_dataset, perf_max_prompt, perf_min_prompt
        ],
        outputs=[perf_logs, perf_progress_status]
    )


def create_interface():
    with gr.Blocks(title='EvalScope Dashboard', theme=gr.themes.Soft()) as demo:
        gr.Markdown('# ğŸš€ EvalScope æœåŠ¡é¢æ¿')

        # Global Service Settings (Top Bar)
        with gr.Accordion('å…¨å±€è®¾ç½®', open=True):
            with gr.Row():
                service_url_input = gr.Textbox(
                    label='EvalScope æœåŠ¡URL', value=DEFAULT_SERVICE_URL, scale=3, info='EvalScopeåç«¯æœåŠ¡çš„è®¿é—®åœ°å€'
                )
                poll_interval_input = gr.Number(label='æ—¥å¿—è½®è¯¢é—´éš” (ç§’)', value=5, minimum=5, scale=1, info='è·å–ä»»åŠ¡æ—¥å¿—çš„é—´éš”æ—¶é—´')

            with gr.Row():
                common_model_name = gr.Textbox(
                    label='æ¨¡å‹åç§°', value='qwen-plus', placeholder='ä¾‹å¦‚: qwen-max, gpt-4', scale=1, info='ç”¨äºè¯„ä¼°æˆ–æ€§èƒ½æµ‹è¯•çš„æ¨¡å‹åç§°'
                )
                common_api_url = gr.Textbox(
                    label='æ¨¡å‹API URL',
                    value='https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions',
                    scale=2,
                    info='æ¨¡å‹APIçš„è¯·æ±‚åœ°å€'
                )
                common_api_key = gr.Textbox(
                    label='æ¨¡å‹API Key',
                    value=os.getenv('DASHSCOPE_API_KEY', ''),
                    type='password',
                    scale=1,
                    info='è®¿é—®æ¨¡å‹APIæ‰€éœ€çš„å¯†é’¥'
                )

        with gr.Tabs():
            with gr.TabItem('æ¨¡å‹è¯„ä¼°'):
                create_eval_interface(
                    service_url_input, poll_interval_input, common_model_name, common_api_url, common_api_key
                )

            with gr.TabItem('æ€§èƒ½æµ‹è¯•'):
                create_perf_interface(
                    service_url_input, poll_interval_input, common_model_name, common_api_url, common_api_key
                )

    return demo


if __name__ == '__main__':
    demo = create_interface()
    demo.queue().launch()
