{
  "meta": {
    "pretty_name": "CrossNER",
    "dataset_id": "extraordinarylab/cross-ner",
    "paper_url": null,
    "tags": [
      "Knowledge",
      "NER"
    ],
    "metrics": [
      "precision",
      "recall",
      "f1_score",
      "accuracy"
    ],
    "few_shot_num": 5,
    "eval_split": "test",
    "train_split": "train",
    "subset_list": [
      "ai",
      "literature",
      "music",
      "politics",
      "science"
    ],
    "description": "## Overview\n\nCrossNER is a fully-labeled collection of named entity recognition (NER) data spanning over five diverse domains: AI, Literature, Music, Politics, and Science. It enables cross-domain NER evaluation and domain adaptation research.\n\n## Task Description\n\n- **Task Type**: Cross-Domain Named Entity Recognition (NER)\n- **Input**: Text from five specialized domains\n- **Output**: Domain-specific entity spans\n- **Domain**: AI, Literature, Music, Politics, Science\n\n## Key Features\n\n- Five diverse domain subsets\n- Domain-specific entity types per subset\n- Enables cross-domain transfer evaluation\n- Fully labeled with expert annotations\n- Useful for domain adaptation research\n\n## Evaluation Notes\n\n- Default configuration uses **5-shot** evaluation\n- Metrics: Precision, Recall, F1-Score, Accuracy\n- Subsets: ai, literature, music, politics, science\n- Entity types vary by domain subset",
    "prompt_template": "You are a named entity recognition system that identifies the following entity types:\n{entities}\n\nProcess the provided text and mark all named entities with XML-style tags.\n\nFor example:\n<person>John Smith</person> works at <organization>Google</organization> in <location>Mountain View</location>.\n\nAvailable entity tags: {entity_list}\n\nINSTRUCTIONS:\n1. Wrap your entire response in <response>...</response> tags.\n2. Inside these tags, include the original text with entity tags inserted.\n3. Do not change the original text in any way (preserve spacing, punctuation, case, etc.).\n4. Tag ALL entities you can identify using the exact tag names provided.\n5. Do not include explanations, just the tagged text.\n6. If entity spans overlap, choose the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\n{text}\n",
    "system_prompt": "",
    "few_shot_prompt_template": "Here are some examples of named entity recognition:\n\n{fewshot}\n\nYou are a named entity recognition system that identifies the following entity types:\n{entities}\n\nProcess the provided text and mark all named entities with XML-style tags.\n\nFor example:\n<person>John Smith</person> works at <organization>Google</organization> in <location>Mountain View</location>.\n\nAvailable entity tags: {entity_list}\n\nINSTRUCTIONS:\n1. Wrap your entire response in <response>...</response> tags.\n2. Inside these tags, include the original text with entity tags inserted.\n3. Do not change the original text in any way (preserve spacing, punctuation, case, etc.).\n4. Tag ALL entities you can identify using the exact tag names provided.\n5. Do not include explanations, just the tagged text.\n6. If entity spans overlap, choose the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\n{text}\n",
    "aggregation": "mean",
    "extra_params": {},
    "sandbox_config": {}
  },
  "statistics": {
    "total_samples": 2506,
    "subset_stats": [
      {
        "name": "ai",
        "sample_count": 431,
        "prompt_length_mean": 5562.3,
        "prompt_length_min": 5407,
        "prompt_length_max": 5878,
        "prompt_length_std": 83.28,
        "target_length_mean": 281.97
      },
      {
        "name": "literature",
        "sample_count": 416,
        "prompt_length_mean": 5725.13,
        "prompt_length_min": 5566,
        "prompt_length_max": 6007,
        "prompt_length_std": 83.39,
        "target_length_mean": 323.98
      },
      {
        "name": "music",
        "sample_count": 465,
        "prompt_length_mean": 5737.0,
        "prompt_length_min": 5570,
        "prompt_length_max": 5962,
        "prompt_length_std": 75.16,
        "target_length_mean": 395.05
      },
      {
        "name": "politics",
        "sample_count": 651,
        "prompt_length_mean": 5701.8,
        "prompt_length_min": 5527,
        "prompt_length_max": 5935,
        "prompt_length_std": 80.5,
        "target_length_mean": 421.93
      },
      {
        "name": "science",
        "sample_count": 543,
        "prompt_length_mean": 5700.71,
        "prompt_length_min": 5548,
        "prompt_length_max": 5995,
        "prompt_length_std": 81.67,
        "target_length_mean": 366.47
      }
    ],
    "prompt_length": {
      "mean": 5687.97,
      "min": 5407,
      "max": 6007,
      "std": 99.96
    },
    "target_length_mean": 364.59,
    "computed_at": "2026-01-28T14:18:50.720739"
  },
  "sample_example": {
    "data": {
      "input": [
        {
          "id": "3a78cbcf",
          "content": "Here are some examples of named entity recognition:\n\nInput:\nPopular approaches of opinion-based recommender system utilize various techniques including text mining , information retrieval , sentiment analysis ( see also Multimodal sentiment a ... [TRUNCATED] ...  the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\nTypical generative model approaches include naive Bayes classifier s , Gaussian mixture model s , variational autoencoders and others .\n"
        }
      ],
      "target": "<response>Typical generative model approaches include <algorithm>naive Bayes classifier</algorithm> s , <algorithm>Gaussian mixture model</algorithm> s , <algorithm>variational autoencoders</algorithm> and others .</response>",
      "id": 0,
      "group_id": 0,
      "metadata": {
        "tokens": [
          "Typical",
          "generative",
          "model",
          "approaches",
          "include",
          "naive",
          "Bayes",
          "classifier",
          "s",
          ",",
          "Gaussian",
          "mixture",
          "model",
          "s",
          ",",
          "variational",
          "autoencoders",
          "and",
          "others",
          "."
        ],
        "ner_tags": [
          "O",
          "O",
          "O",
          "O",
          "O",
          "B-ALGORITHM",
          "I-ALGORITHM",
          "I-ALGORITHM",
          "O",
          "O",
          "B-ALGORITHM",
          "I-ALGORITHM",
          "I-ALGORITHM",
          "O",
          "O",
          "B-ALGORITHM",
          "I-ALGORITHM",
          "O",
          "O",
          "O"
        ]
      }
    },
    "subset": "ai",
    "truncated": true
  },
  "readme": {
    "en": "# CrossNER\n\n## Overview\n\nCrossNER is a fully-labeled collection of named entity recognition (NER) data spanning over five diverse domains: AI, Literature, Music, Politics, and Science. It enables cross-domain NER evaluation and domain adaptation research.\n\n## Task Description\n\n- **Task Type**: Cross-Domain Named Entity Recognition (NER)\n- **Input**: Text from five specialized domains\n- **Output**: Domain-specific entity spans\n- **Domain**: AI, Literature, Music, Politics, Science\n\n## Key Features\n\n- Five diverse domain subsets\n- Domain-specific entity types per subset\n- Enables cross-domain transfer evaluation\n- Fully labeled with expert annotations\n- Useful for domain adaptation research\n\n## Evaluation Notes\n\n- Default configuration uses **5-shot** evaluation\n- Metrics: Precision, Recall, F1-Score, Accuracy\n- Subsets: ai, literature, music, politics, science\n- Entity types vary by domain subset\n\n## Properties\n\n| Property | Value |\n|----------|-------|\n| **Benchmark Name** | `cross_ner` |\n| **Dataset ID** | [extraordinarylab/cross-ner](https://modelscope.cn/datasets/extraordinarylab/cross-ner/summary) |\n| **Paper** | N/A |\n| **Tags** | `Knowledge`, `NER` |\n| **Metrics** | `precision`, `recall`, `f1_score`, `accuracy` |\n| **Default Shots** | 5-shot |\n| **Evaluation Split** | `test` |\n| **Train Split** | `train` |\n\n\n## Data Statistics\n\n| Metric | Value |\n|--------|-------|\n| Total Samples | 2,506 |\n| Prompt Length (Mean) | 5687.97 chars |\n| Prompt Length (Min/Max) | 5407 / 6007 chars |\n\n**Per-Subset Statistics:**\n\n| Subset | Samples | Prompt Mean | Prompt Min | Prompt Max |\n|--------|---------|-------------|------------|------------|\n| `ai` | 431 | 5562.3 | 5407 | 5878 |\n| `literature` | 416 | 5725.13 | 5566 | 6007 |\n| `music` | 465 | 5737.0 | 5570 | 5962 |\n| `politics` | 651 | 5701.8 | 5527 | 5935 |\n| `science` | 543 | 5700.71 | 5548 | 5995 |\n\n## Sample Example\n\n**Subset**: `ai`\n\n```json\n{\n  \"input\": [\n    {\n      \"id\": \"3a78cbcf\",\n      \"content\": \"Here are some examples of named entity recognition:\\n\\nInput:\\nPopular approaches of opinion-based recommender system utilize various techniques including text mining , information retrieval , sentiment analysis ( see also Multimodal sentiment a ... [TRUNCATED] ...  the most specific entity type.\\n7. Ensure every opening tag has a matching closing tag.\\n\\nText to process:\\nTypical generative model approaches include naive Bayes classifier s , Gaussian mixture model s , variational autoencoders and others .\\n\"\n    }\n  ],\n  \"target\": \"<response>Typical generative model approaches include <algorithm>naive Bayes classifier</algorithm> s , <algorithm>Gaussian mixture model</algorithm> s , <algorithm>variational autoencoders</algorithm> and others .</response>\",\n  \"id\": 0,\n  \"group_id\": 0,\n  \"metadata\": {\n    \"tokens\": [\n      \"Typical\",\n      \"generative\",\n      \"model\",\n      \"approaches\",\n      \"include\",\n      \"naive\",\n      \"Bayes\",\n      \"classifier\",\n      \"s\",\n      \",\",\n      \"Gaussian\",\n      \"mixture\",\n      \"model\",\n      \"s\",\n      \",\",\n      \"variational\",\n      \"autoencoders\",\n      \"and\",\n      \"others\",\n      \".\"\n    ],\n    \"ner_tags\": [\n      \"O\",\n      \"O\",\n      \"O\",\n      \"O\",\n      \"O\",\n      \"B-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"O\",\n      \"O\",\n      \"B-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"O\",\n      \"O\",\n      \"B-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"O\",\n      \"O\",\n      \"O\"\n    ]\n  }\n}\n```\n\n*Note: Some content was truncated for display.*\n\n## Prompt Template\n\n**Prompt Template:**\n```text\nYou are a named entity recognition system that identifies the following entity types:\n{entities}\n\nProcess the provided text and mark all named entities with XML-style tags.\n\nFor example:\n<person>John Smith</person> works at <organization>Google</organization> in <location>Mountain View</location>.\n\nAvailable entity tags: {entity_list}\n\nINSTRUCTIONS:\n1. Wrap your entire response in <response>...</response> tags.\n2. Inside these tags, include the original text with entity tags inserted.\n3. Do not change the original text in any way (preserve spacing, punctuation, case, etc.).\n4. Tag ALL entities you can identify using the exact tag names provided.\n5. Do not include explanations, just the tagged text.\n6. If entity spans overlap, choose the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\n{text}\n\n```\n\n<details>\n<summary>Few-shot Template</summary>\n\n```text\nHere are some examples of named entity recognition:\n\n{fewshot}\n\nYou are a named entity recognition system that identifies the following entity types:\n{entities}\n\nProcess the provided text and mark all named entities with XML-style tags.\n\nFor example:\n<person>John Smith</person> works at <organization>Google</organization> in <location>Mountain View</location>.\n\nAvailable entity tags: {entity_list}\n\nINSTRUCTIONS:\n1. Wrap your entire response in <response>...</response> tags.\n2. Inside these tags, include the original text with entity tags inserted.\n3. Do not change the original text in any way (preserve spacing, punctuation, case, etc.).\n4. Tag ALL entities you can identify using the exact tag names provided.\n5. Do not include explanations, just the tagged text.\n6. If entity spans overlap, choose the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\n{text}\n\n```\n\n</details>\n\n## Usage\n\n### Using CLI\n\n```bash\nevalscope eval \\\n    --model YOUR_MODEL \\\n    --api-url OPENAI_API_COMPAT_URL \\\n    --api-key EMPTY_TOKEN \\\n    --datasets cross_ner \\\n    --limit 10  # Remove this line for formal evaluation\n```\n\n### Using Python\n\n```python\nfrom evalscope import run_task\nfrom evalscope.config import TaskConfig\n\ntask_cfg = TaskConfig(\n    model='YOUR_MODEL',\n    api_url='OPENAI_API_COMPAT_URL',\n    api_key='EMPTY_TOKEN',\n    datasets=['cross_ner'],\n    dataset_args={\n        'cross_ner': {\n            # subset_list: ['ai', 'literature', 'music']  # optional, evaluate specific subsets\n        }\n    },\n    limit=10,  # Remove this line for formal evaluation\n)\n\nrun_task(task_cfg=task_cfg)\n```\n\n\n",
    "zh": "# CrossNER\n\n## 概述\n\nCrossNER 是一个完全标注的命名实体识别（NER）数据集，涵盖五个不同领域：人工智能（AI）、文学（Literature）、音乐（Music）、政治（Politics）和科学（Science）。该数据集支持跨领域 NER 评估和领域自适应研究。\n\n## 任务描述\n\n- **任务类型**：跨领域命名实体识别（NER）\n- **输入**：来自五个专业领域的文本\n- **输出**：领域特定的实体片段\n- **领域**：AI、文学、音乐、政治、科学\n\n## 主要特点\n\n- 包含五个多样化的领域子集\n- 每个子集具有领域特定的实体类型\n- 支持跨领域迁移能力评估\n- 由专家完全标注\n- 适用于领域自适应研究\n\n## 评估说明\n\n- 默认配置使用 **5-shot** 评估\n- 评估指标：精确率（Precision）、召回率（Recall）、F1 分数（F1-Score）、准确率（Accuracy）\n- 子集：ai、literature、music、politics、science\n- 实体类型因领域子集而异\n\n## 属性\n\n| 属性 | 值 |\n|----------|-------|\n| **基准测试名称** | `cross_ner` |\n| **数据集 ID** | [extraordinarylab/cross-ner](https://modelscope.cn/datasets/extraordinarylab/cross-ner/summary) |\n| **论文** | 无 |\n| **标签** | `Knowledge`, `NER` |\n| **指标** | `precision`, `recall`, `f1_score`, `accuracy` |\n| **默认样本数** | 5-shot |\n| **评估划分** | `test` |\n| **训练划分** | `train` |\n\n## 数据统计\n\n| 指标 | 值 |\n|--------|-------|\n| 总样本数 | 2,506 |\n| 提示词长度（平均） | 5687.97 字符 |\n| 提示词长度（最小/最大） | 5407 / 6007 字符 |\n\n**各子集统计数据：**\n\n| 子集 | 样本数 | 提示词平均长度 | 提示词最小长度 | 提示词最大长度 |\n|--------|---------|-------------|------------|------------|\n| `ai` | 431 | 5562.3 | 5407 | 5878 |\n| `literature` | 416 | 5725.13 | 5566 | 6007 |\n| `music` | 465 | 5737.0 | 5570 | 5962 |\n| `politics` | 651 | 5701.8 | 5527 | 5935 |\n| `science` | 543 | 5700.71 | 5548 | 5995 |\n\n## 样例示例\n\n**子集**: `ai`\n\n```json\n{\n  \"input\": [\n    {\n      \"id\": \"3a78cbcf\",\n      \"content\": \"Here are some examples of named entity recognition:\\n\\nInput:\\nPopular approaches of opinion-based recommender system utilize various techniques including text mining , information retrieval , sentiment analysis ( see also Multimodal sentiment a ... [TRUNCATED] ...  the most specific entity type.\\n7. Ensure every opening tag has a matching closing tag.\\n\\nText to process:\\nTypical generative model approaches include naive Bayes classifier s , Gaussian mixture model s , variational autoencoders and others .\\n\"\n    }\n  ],\n  \"target\": \"<response>Typical generative model approaches include <algorithm>naive Bayes classifier</algorithm> s , <algorithm>Gaussian mixture model</algorithm> s , <algorithm>variational autoencoders</algorithm> and others .</response>\",\n  \"id\": 0,\n  \"group_id\": 0,\n  \"metadata\": {\n    \"tokens\": [\n      \"Typical\",\n      \"generative\",\n      \"model\",\n      \"approaches\",\n      \"include\",\n      \"naive\",\n      \"Bayes\",\n      \"classifier\",\n      \"s\",\n      \",\",\n      \"Gaussian\",\n      \"mixture\",\n      \"model\",\n      \"s\",\n      \",\",\n      \"variational\",\n      \"autoencoders\",\n      \"and\",\n      \"others\",\n      \".\"\n    ],\n    \"ner_tags\": [\n      \"O\",\n      \"O\",\n      \"O\",\n      \"O\",\n      \"O\",\n      \"B-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"O\",\n      \"O\",\n      \"B-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"O\",\n      \"O\",\n      \"B-ALGORITHM\",\n      \"I-ALGORITHM\",\n      \"O\",\n      \"O\",\n      \"O\"\n    ]\n  }\n}\n```\n\n*注：部分内容为显示目的已截断。*\n\n## 提示模板\n\n**提示模板：**\n```text\nYou are a named entity recognition system that identifies the following entity types:\n{entities}\n\nProcess the provided text and mark all named entities with XML-style tags.\n\nFor example:\n<person>John Smith</person> works at <organization>Google</organization> in <location>Mountain View</location>.\n\nAvailable entity tags: {entity_list}\n\nINSTRUCTIONS:\n1. Wrap your entire response in <response>...</response> tags.\n2. Inside these tags, include the original text with entity tags inserted.\n3. Do not change the original text in any way (preserve spacing, punctuation, case, etc.).\n4. Tag ALL entities you can identify using the exact tag names provided.\n5. Do not include explanations, just the tagged text.\n6. If entity spans overlap, choose the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\n{text}\n\n```\n\n<details>\n<summary>少样本（Few-shot）模板</summary>\n\n```text\nHere are some examples of named entity recognition:\n\n{fewshot}\n\nYou are a named entity recognition system that identifies the following entity types:\n{entities}\n\nProcess the provided text and mark all named entities with XML-style tags.\n\nFor example:\n<person>John Smith</person> works at <organization>Google</organization> in <location>Mountain View</location>.\n\nAvailable entity tags: {entity_list}\n\nINSTRUCTIONS:\n1. Wrap your entire response in <response>...</response> tags.\n2. Inside these tags, include the original text with entity tags inserted.\n3. Do not change the original text in any way (preserve spacing, punctuation, case, etc.).\n4. Tag ALL entities you can identify using the exact tag names provided.\n5. Do not include explanations, just the tagged text.\n6. If entity spans overlap, choose the most specific entity type.\n7. Ensure every opening tag has a matching closing tag.\n\nText to process:\n{text}\n\n```\n\n</details>\n\n## 使用方法\n\n### 使用命令行（CLI）\n\n```bash\nevalscope eval \\\n    --model YOUR_MODEL \\\n    --api-url OPENAI_API_COMPAT_URL \\\n    --api-key EMPTY_TOKEN \\\n    --datasets cross_ner \\\n    --limit 10  # 正式评估时请删除此行\n```\n\n### 使用 Python\n\n```python\nfrom evalscope import run_task\nfrom evalscope.config import TaskConfig\n\ntask_cfg = TaskConfig(\n    model='YOUR_MODEL',\n    api_url='OPENAI_API_COMPAT_URL',\n    api_key='EMPTY_TOKEN',\n    datasets=['cross_ner'],\n    dataset_args={\n        'cross_ner': {\n            # subset_list: ['ai', 'literature', 'music']  # 可选，用于评估特定子集\n        }\n    },\n    limit=10,  # 正式评估时请删除此行\n)\n\nrun_task(task_cfg=task_cfg)\n```",
    "content_hash": "b17526c5308e9192fb50fe0e1dabb276",
    "needs_translation": false
  },
  "updated_at": "2026-01-28T14:29:35.191386",
  "translation_updated_at": "2026-01-28T16:09:53Z"
}
