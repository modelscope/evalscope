# Copyright (c) Alibaba, Inc. and its affiliates.

import re
from typing import Any, Dict

from evalscope.api.benchmark import BenchmarkMeta, DefaultDataAdapter
from evalscope.api.dataset import Sample
from evalscope.api.evaluator import TaskState
from evalscope.api.metric import Score
from evalscope.api.registry import register_benchmark
from evalscope.constants import Tags
from evalscope.utils.logger import get_logger

logger = get_logger()


@register_benchmark(
    BenchmarkMeta(
        name='math_500',
        pretty_name='MATH-500',
        tags=[Tags.MATH, Tags.REASONING],
        description=
        "MATH-500 is a benchmark for evaluating mathematical reasoning capabilities of AI models. It consists of 500 diverse math problems across five levels of difficulty, designed to test a model's ability to solve complex mathematical problems by generating step-by-step solutions and providing the correct final answer.",  # noqa: E501
        dataset_id='AI-ModelScope/MATH-500',
        subset_list=['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5'],
        metric_list=[{
            'acc': {}
        }],  # ä¸ä½¿ç”¨ numericï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ extract_answer ä¸­æå–äº†
        few_shot_num=0,
        train_split=None,
        eval_split='test',
        prompt_template='{question}\nPlease reason step by step, and put your final answer within \\boxed{{}}.',
    )
)
class Math500Adapter(DefaultDataAdapter):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.reformat_subset = True

    def record_to_sample(self, record: Dict[str, Any]) -> Sample:
        return Sample(
            input=record['problem'],
            target=record['answer'],
            subset_key=f"Level {record['level']}",
            metadata={
                'question_id': record['unique_id'],
                'solution': record['solution'],
            },
        )

    def extract_answer(self, prediction: str, task_state: TaskState) -> str:
        """
        æå–ç­”æ¡ˆï¼Œæ”¯æŒä»£ç å—å’Œæ•°å­¦å…¬å¼çš„æå–ã€‚
        ä¼˜å…ˆçº§ï¼šä»£ç å— > boxedå…¬å¼ > å…¶ä»–æ ¼å¼

        æ”¯æŒçš„ä»£ç å—æ ¼å¼ï¼š
        - ```python ... ```
        - ```math ... ```
        - ```latex ... ```
        - ``` ... ``` (æ— è¯­è¨€æ ‡è®°)
        """
        from evalscope.metrics.math_parser import extract_answer as math_extract_answer

        # DEBUG: æ·»åŠ æ—¥å¿—
        print(f"\n{'='*80}")
        print('ğŸ”ğŸ”ğŸ” [DEBUG] Math500Adapter.extract_answer è¢«è°ƒç”¨ï¼')
        print(f'é¢„æµ‹é•¿åº¦: {len(prediction)}')
        print(f'é¢„æµ‹å‰100å­—ç¬¦: {prediction[:100]}')
        print(f"{'='*80}\n")

        # 1. å°è¯•æå–ä»£ç å—ä¸­çš„å†…å®¹ï¼ˆæ”¯æŒå¤šç§è¯­è¨€æ ‡è®°ï¼ŒåŒ…æ‹¬æ— æ ‡è®°çš„æƒ…å†µï¼‰
        code_blocks = re.findall(r'```(?:\w+)?\s*\n(.*?)```', prediction, re.DOTALL)
        if code_blocks:
            # å¦‚æœæœ‰å¤šä¸ªä»£ç å—ï¼Œå–æœ€åä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
            code_content = code_blocks[-1].strip()
            # ä»ä»£ç å—ä¸­æå–ç­”æ¡ˆ
            extracted = math_extract_answer(code_content)
            if extracted:
                return extracted

        # 2. å°è¯•æå–å•è¡Œä»£ç æ ¼å¼ï¼ˆå¦‚ï¼š`answer`ï¼‰
        inline_code = re.findall(r'`([^`]+)`', prediction)
        if inline_code:
            # å–æœ€åä¸€ä¸ªå†…è”ä»£ç 
            inline_content = inline_code[-1].strip()
            extracted = math_extract_answer(inline_content)
            if extracted:
                return extracted

        # 3. å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œä½¿ç”¨åŸæœ‰çš„æå–é€»è¾‘ï¼ˆæ”¯æŒ \boxed{}, "ç­”æ¡ˆæ˜¯" ç­‰æ ¼å¼ï¼‰
        result = math_extract_answer(prediction)

        print(f"\n{'='*80}")
        print('âœ…âœ…âœ… [DEBUG] Math500Adapter.extract_answer è¿”å›ï¼')
        print(f"è¿”å›ç»“æœ: '{result}'")
        print(f'è¿”å›ç»“æœé•¿åº¦: {len(result)}')
        print(f"{'='*80}\n")

        return result

    def match_score(
        self, original_prediction: str, filtered_prediction: str, reference: str, task_state: TaskState
    ) -> Score:
        """
        ä½¿ç”¨ math_equal æ¯”è¾ƒå·²æå–çš„ç­”æ¡ˆã€‚
        ä¸å†æ¬¡è°ƒç”¨ extract_answerï¼Œå› ä¸º filtered_prediction å·²ç»æ˜¯æå–åçš„ç­”æ¡ˆã€‚
        """
        from evalscope.metrics.math_parser import math_equal, strip_answer_string

        # filtered_prediction å·²ç»æ˜¯æå–åçš„ç­”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨
        pred_answer = strip_answer_string(filtered_prediction)
        ref_answer = strip_answer_string(reference)

        # ä½¿ç”¨ math_equal è¿›è¡Œæ•°å­¦è¡¨è¾¾å¼çš„ç¬¦å·æ¯”è¾ƒ
        is_correct = math_equal(pred_answer, ref_answer)

        score = Score(
            extracted_prediction=filtered_prediction, prediction=original_prediction, value={'acc': float(is_correct)}
        )

        return score
