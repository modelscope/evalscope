# Copyright (c) Alibaba, Inc. and its affiliates.
"""
Utility functions for generating README documentation for benchmarks.

This module provides tools to automatically generate comprehensive README.md
files for benchmark datasets, including metadata, statistics, examples,
and usage instructions.
"""

import os
from datetime import datetime
from typing import TYPE_CHECKING, Dict, List, Optional, Union

from evalscope.utils.logger import get_logger

if TYPE_CHECKING:
    from evalscope.api.benchmark import DataAdapter
    from evalscope.api.benchmark.statistics import DataStatistics, SampleExample

logger = get_logger()

# Standard README template with all sections
README_TEMPLATE = '''# {pretty_name}

{description}

## Overview

| Property | Value |
|----------|-------|
| **Benchmark Name** | `{name}` |
| **Dataset ID** | {dataset_id_link} |
| **Paper** | {paper_link} |
| **Tags** | {tags} |
| **Metrics** | {metrics} |
| **Default Shots** | {few_shot_num}-shot |
| **Evaluation Split** | `{eval_split}` |

## Data Statistics

{statistics_section}

## Subsets

{subsets_section}

## Sample Example

{sample_example_section}

## Prompt Template

{prompt_template_section}

## Usage

```python
from evalscope import run_task

results = run_task(
    model='your-model',
    datasets=['{name}'],
)
```

---
*Auto-generated by EvalScope on {generated_at}*
'''


def _format_dataset_link(dataset_id: str) -> str:
    """Format dataset ID as a clickable link."""
    if not dataset_id:
        return 'N/A'
    if dataset_id.startswith(('http://', 'https://')):
        return f'[{os.path.basename(dataset_id)}]({dataset_id})'
    elif '/' in dataset_id:
        # ModelScope format: org/name
        return f'[{dataset_id}](https://modelscope.cn/datasets/{dataset_id}/summary)'
    return f'`{dataset_id}`'


def _format_link(url: Optional[str], display_text: str = 'Link') -> str:
    """Format a URL as a markdown link."""
    if url:
        return f'[{display_text}]({url})'
    return 'N/A'


def _format_tags(tags: List[str]) -> str:
    """Format tags as inline code."""
    if not tags:
        return 'N/A'
    return ', '.join(f'`{t}`' for t in sorted(tags))


def _format_metrics(metric_list: List[Union[str, Dict]]) -> str:
    """Format metrics list."""
    if not metric_list:
        return 'N/A'

    formatted = []
    for m in metric_list:
        if isinstance(m, str):
            formatted.append(f'`{m}`')
        elif isinstance(m, dict):
            # Handle dict format like {'acc': {'numeric': True}}
            for key in m.keys():
                formatted.append(f'`{key}`')
    return ', '.join(formatted)


def _format_statistics_section(stats: Optional['DataStatistics']) -> str:
    """Format the statistics section."""
    if not stats:
        return '*Statistics not available. Run `evalscope benchmark-info --compute-stats` to generate.*'
    return stats.to_markdown_table()


def _format_subsets_section(subset_list: List[str], stats: Optional['DataStatistics'] = None) -> str:
    """Format the subsets section."""
    if not subset_list:
        return '*No subsets defined.*'

    lines = []
    if len(subset_list) == 1 and subset_list[0] in ('default', 'main'):
        lines.append(f'- `{subset_list[0]}` (default)')
    else:
        lines.append('| Subset | Sample Count |')
        lines.append('|--------|--------------|')
        for subset in subset_list:
            # Try to get sample count from statistics
            count = 'N/A'
            if stats and stats.subset_stats:
                for s in stats.subset_stats:
                    if s.name == subset:
                        count = f'{s.sample_count:,}'
                        break
            lines.append(f'| `{subset}` | {count} |')

    return '\n'.join(lines)


def _format_sample_example_section(example: Optional['SampleExample']) -> str:
    """Format the sample example section."""
    if not example:
        return '*Sample example not available. Run `evalscope benchmark-info --compute-stats` to generate.*'
    return example.to_json_block()


def _format_prompt_template_section(
    prompt_template: Optional[str],
    system_prompt: Optional[str] = None,
    few_shot_template: Optional[str] = None,
) -> str:
    """Format the prompt template section."""
    sections = []

    if system_prompt:
        sections.append('**System Prompt:**')
        sections.append('```text')
        sections.append(system_prompt)
        sections.append('```')
        sections.append('')

    if prompt_template:
        sections.append('**Prompt Template:**')
        sections.append('```text')
        sections.append(prompt_template)
        sections.append('```')
    else:
        sections.append('*No prompt template defined.*')

    if few_shot_template:
        sections.append('')
        sections.append('<details>')
        sections.append('<summary>Few-shot Template</summary>')
        sections.append('')
        sections.append('```text')
        sections.append(few_shot_template)
        sections.append('```')
        sections.append('')
        sections.append('</details>')

    return '\n'.join(sections)


def generate_benchmark_readme(
    adapter: 'DataAdapter',
    include_statistics: bool = True,
    include_example: bool = True,
    compute_if_missing: bool = False,
    use_persistent_storage: bool = True,
) -> str:
    """
    Generate a comprehensive README.md for a benchmark.

    This function creates a well-formatted README with all relevant benchmark
    information including metadata, statistics, examples, and usage instructions.

    Args:
        adapter: The DataAdapter instance for the benchmark.
        include_statistics: Whether to include data statistics section.
        include_example: Whether to include sample example section.
        compute_if_missing: Whether to compute statistics/examples if not present.
        use_persistent_storage: Whether to use persistent storage for statistics.

    Returns:
        Formatted README markdown string.

    Example:
        >>> from evalscope.api.registry import get_benchmark
        >>> adapter = get_benchmark('gsm8k')
        >>> readme = generate_benchmark_readme(adapter, compute_if_missing=True)
        >>> print(readme)
    """
    meta = adapter._benchmark_meta

    # Optionally load/compute statistics and examples
    if compute_if_missing:
        if use_persistent_storage:
            from evalscope.utils.benchmark_stats import load_or_compute_statistics
            load_or_compute_statistics(adapter)
        else:
            from evalscope.utils.benchmark_stats import compute_and_attach_statistics
            compute_and_attach_statistics(adapter)

    # Prepare all sections
    stats = meta.data_statistics if include_statistics else None
    example = meta.sample_example if include_example else None

    return README_TEMPLATE.format(
        pretty_name=meta.pretty_name or meta.name,
        description=meta.description or '*No description available.*',
        name=meta.name,
        dataset_id_link=_format_dataset_link(meta.dataset_id),
        paper_link=_format_link(meta.paper_url, 'Paper'),
        tags=_format_tags(meta.tags),
        metrics=_format_metrics(meta.metric_list),
        few_shot_num=meta.few_shot_num,
        eval_split=meta.eval_split or 'N/A',
        statistics_section=_format_statistics_section(stats),
        subsets_section=_format_subsets_section(meta.subset_list, stats),
        sample_example_section=_format_sample_example_section(example),
        prompt_template_section=_format_prompt_template_section(
            meta.prompt_template,
            meta.system_prompt,
            meta.few_shot_prompt_template,
        ),
        generated_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    )


def save_benchmark_readme(
    adapter: 'DataAdapter',
    output_path: Optional[str] = None,
    include_statistics: bool = True,
    include_example: bool = True,
    compute_if_missing: bool = False,
) -> str:
    """
    Generate and save a README.md for a benchmark.

    Args:
        adapter: The DataAdapter instance for the benchmark.
        output_path: Path to save the README. If None, saves to benchmark directory.
        include_statistics: Whether to include data statistics section.
        include_example: Whether to include sample example section.
        compute_if_missing: Whether to compute statistics/examples if not present.

    Returns:
        Path to the saved README file.
    """
    readme_content = generate_benchmark_readme(
        adapter,
        include_statistics=include_statistics,
        include_example=include_example,
        compute_if_missing=compute_if_missing,
    )

    if output_path is None:
        # Default to benchmark directory
        import evalscope.benchmarks as benchmarks_module

        benchmarks_dir = os.path.dirname(benchmarks_module.__file__)
        benchmark_dir = os.path.join(benchmarks_dir, adapter.name.replace('-', '_'))
        os.makedirs(benchmark_dir, exist_ok=True)
        output_path = os.path.join(benchmark_dir, 'README.md')

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)

    logger.info(f'README saved to: {output_path}')
    return output_path


def generate_all_benchmark_readmes(
    output_dir: Optional[str] = None,
    benchmarks: Optional[List[str]] = None,
    compute_statistics: bool = False,
) -> Dict[str, str]:
    """
    Generate README files for all registered benchmarks.

    Args:
        output_dir: Directory to save README files. If None, saves to each benchmark's directory.
        benchmarks: List of benchmark names to generate. If None, generates for all.
        compute_statistics: Whether to compute statistics for benchmarks.

    Returns:
        Dictionary mapping benchmark names to README file paths.
    """
    from evalscope.api.registry import BENCHMARK_REGISTRY, get_benchmark

    results = {}
    benchmark_names = benchmarks or list(BENCHMARK_REGISTRY.keys())

    for name in benchmark_names:
        try:
            logger.info(f'Generating README for: {name}')
            adapter = get_benchmark(name)

            if output_dir:
                readme_path = os.path.join(output_dir, f'{name}.md')
            else:
                readme_path = None

            saved_path = save_benchmark_readme(
                adapter,
                output_path=readme_path,
                compute_if_missing=compute_statistics,
            )
            results[name] = saved_path

        except Exception as e:
            logger.error(f'Failed to generate README for {name}: {e}')
            results[name] = f'ERROR: {e}'

    return results
