# ğŸ‘ è´¡çŒ®åŸºå‡†è¯„æµ‹

EvalScopeä½œä¸º[ModelScope](https://modelscope.cn)çš„å®˜æ–¹è¯„æµ‹å·¥å…·ï¼Œå…¶åŸºå‡†è¯„æµ‹åŠŸèƒ½æ­£åœ¨æŒç»­ä¼˜åŒ–ä¸­ï¼æˆ‘ä»¬è¯šé‚€æ‚¨å‚è€ƒæœ¬æ•™ç¨‹ï¼Œè½»æ¾æ·»åŠ è‡ªå·±çš„è¯„æµ‹åŸºå‡†ï¼Œå¹¶ä¸å¹¿å¤§ç¤¾åŒºæˆå‘˜åˆ†äº«æ‚¨çš„è´¡çŒ®ã€‚ä¸€èµ·åŠ©åŠ›EvalScopeçš„æˆé•¿ï¼Œè®©æˆ‘ä»¬çš„å·¥å…·æ›´åŠ å‡ºè‰²ï¼

ä¸‹é¢ä»¥`MMLU-Pro`ä¸ºä¾‹ï¼Œä»‹ç»å¦‚ä½•æ·»åŠ åŸºå‡†è¯„æµ‹ï¼Œä¸»è¦åŒ…å«ä¸Šä¼ æ•°æ®é›†ã€æ³¨å†Œæ•°æ®é›†ã€ç¼–å†™è¯„æµ‹ä»»åŠ¡ä¸‰ä¸ªæ­¥éª¤ã€‚

## 1. å‡†å¤‡åŸºå‡†è¯„æµ‹æ•°æ®é›†

æ‚¨æœ‰ä¸¤ç§æ–¹å¼å‡†å¤‡åŸºå‡†è¯„æµ‹æ•°æ®é›†ï¼š

1. **ä¸Šä¼ åˆ°ModelScopeï¼ˆæ¨èï¼‰**ï¼šå°†æ•°æ®é›†ä¸Šä¼ åˆ°ModelScopeå¹³å°ï¼Œè¿™æ ·å…¶ä»–ç”¨æˆ·å¯ä»¥ä¸€é”®åŠ è½½æ‚¨çš„æ•°æ®é›†ï¼Œä½¿ç”¨æ›´åŠ ä¾¿æ·ï¼Œä¹Ÿèƒ½è®©æ›´å¤šç”¨æˆ·å—ç›Šäºæ‚¨çš„è´¡çŒ®ã€‚

2. **æœ¬åœ°ä½¿ç”¨**ï¼šæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨æœ¬åœ°æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼Œé€‚åˆæ•°æ®é›†å°šåœ¨å¼€å‘é˜¶æ®µæˆ–å«æœ‰æ•æ„Ÿä¿¡æ¯çš„æƒ…å†µã€‚

```{seealso}
å¦‚éœ€ä¸Šä¼ åˆ°ModelScopeï¼Œå¯å‚è€ƒï¼š[modelscope/MMLU-Pro](https://modelscope.cn/datasets/modelscope/MMLU-Pro/summary)ç¤ºä¾‹ï¼Œä»¥åŠ[æ•°æ®é›†ä¸Šä¼ æ•™ç¨‹](https://www.modelscope.cn/docs/datasets/create)ã€‚
```

æ— è®ºé€‰æ‹©å“ªç§æ–¹å¼ï¼Œè¯·ç¡®ä¿æ•°æ®çš„æ ¼å¼æ­£ç¡®ä¸”å¯è¢«åŠ è½½ã€‚å¦‚ä½¿ç”¨ModelScopeï¼Œå¯é€šè¿‡ä»¥ä¸‹ä»£ç æµ‹è¯•ï¼š

```python
from modelscope import MsDataset

dataset = MsDataset.load("modelscope/MMLU-Pro")  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†
```

è‹¥ä½¿ç”¨æœ¬åœ°æ•°æ®é›†ï¼Œéœ€è¦åœ¨åç»­æ³¨å†ŒåŸºå‡†è¯„æµ‹æ—¶ç›¸åº”è°ƒæ•´`dataset_id`å‚æ•°å’Œé‡å†™`load_from_disk`æ–¹æ³•ã€‚

## 2. æ³¨å†ŒåŸºå‡†è¯„æµ‹

åœ¨EvalScopeä¸­æ·»åŠ åŸºå‡†è¯„æµ‹ã€‚

### åˆ›å»ºæ–‡ä»¶ç»“æ„

é¦–å…ˆ[Fork EvalScope](https://github.com/modelscope/evalscope/fork) ä»“åº“ï¼Œå³åˆ›å»ºä¸€ä¸ªè‡ªå·±çš„EvalScopeä»“åº“å‰¯æœ¬ï¼Œå°†å…¶cloneåˆ°æœ¬åœ°ã€‚

ç„¶åï¼Œåœ¨`evalscope/benchmarks/`ç›®å½•ä¸‹æ·»åŠ åŸºå‡†è¯„æµ‹ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
evalscope/benchmarks/
â”œâ”€â”€ benchmark_name
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ benchmark_name_adapter.py
â”‚   â””â”€â”€ ...
```
å…·ä½“åˆ°`MMLU-Pro`ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
evalscope/benchmarks/
â”œâ”€â”€ mmlu_pro
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mmlu_pro_adapter.py
â”‚   â””â”€â”€ ...
```

### æ³¨å†Œ`Benchmark`

æˆ‘ä»¬éœ€è¦åœ¨`benchmark_name_adapter.py`ä¸­æ³¨å†Œ`Benchmark`ï¼Œä½¿å¾—EvalScopeèƒ½å¤ŸåŠ è½½æˆ‘ä»¬æ·»åŠ çš„åŸºå‡†æµ‹è¯•ã€‚ä»¥`MMLU-Pro`ä¸ºä¾‹ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

- å¯¼å…¥`Benchmark`å’Œ`DataAdapter`
- æ³¨å†Œ`Benchmark`ï¼ŒæŒ‡å®šï¼š
    - `name`ï¼šåŸºå‡†æµ‹è¯•åç§°
    - `pretty_name`ï¼šåŸºå‡†æµ‹è¯•çš„å¯è¯»åç§°
    - `tags`ï¼šåŸºå‡†æµ‹è¯•æ ‡ç­¾ï¼Œç”¨äºåˆ†ç±»å’Œæœç´¢
    - `description`ï¼šåŸºå‡†æµ‹è¯•æè¿°ï¼Œå¯ä»¥ä½¿ç”¨Markdownæ ¼å¼ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡
    - `dataset_id`ï¼šåŸºå‡†æµ‹è¯•æ•°æ®é›†IDï¼Œç”¨äºåŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®é›†
    - `model_adapter`ï¼šåŸºå‡†æµ‹è¯•æ¨¡å‹é»˜è®¤é€‚é…å™¨ã€‚æ”¯æŒä¸¤ç§ï¼š
        - `OutputType.GENERATION`ï¼šé€šç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹è¯„æµ‹ï¼Œé€šè¿‡è¾“å…¥promptï¼Œè¿”å›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        - `OutputType.MULTIPLE_CHOICE`ï¼šå¤šé€‰é¢˜è¯„æµ‹ï¼Œé€šè¿‡logitsæ¥è®¡ç®—é€‰é¡¹çš„æ¦‚ç‡ï¼Œè¿”å›æœ€å¤§æ¦‚ç‡é€‰é¡¹
    - `output_types`ï¼šåŸºå‡†æµ‹è¯•è¾“å‡ºç±»å‹ï¼Œæ”¯æŒå¤šé€‰ï¼š
        - `OutputType.GENERATION`ï¼šé€šç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹è¯„æµ‹
        - `OutputType.MULTIPLE_CHOICE`ï¼šå¤šé€‰é¢˜è¯„æµ‹è¾“å‡ºlogits
    - `subset_list`ï¼šåŸºå‡†æµ‹è¯•æ•°æ®é›†çš„å­æ•°æ®é›†
    - `metric_list`ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼°æŒ‡æ ‡
    - `few_shot_num`ï¼šè¯„æµ‹çš„In Context Learningæ ·æœ¬æ•°é‡
    - `train_split`ï¼šåŸºå‡†æµ‹è¯•è®­ç»ƒé›†ï¼Œç”¨äºé‡‡æ ·ICLæ ·ä¾‹
    - `eval_split`ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼°é›†
    - `prompt_template`ï¼šåŸºå‡†æµ‹è¯•æç¤ºæ¨¡æ¿
- åˆ›å»º`MMLUProAdapter`ç±»ï¼Œç»§æ‰¿è‡ª`DataAdapter`ã€‚

```{tip}
é»˜è®¤`subset_list`, `train_split`, `eval_split` å¯ä»¥ä»æ•°æ®é›†é¢„è§ˆä¸­è·å–ï¼Œä¾‹å¦‚[MMLU-Proé¢„è§ˆ](https://modelscope.cn/datasets/modelscope/MMLU-Pro/dataPeview)

![MMLU-Proé¢„è§ˆ](./images/mmlu_pro_preview.png)
```

ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from evalscope.benchmarks import Benchmark, DataAdapter
from evalscope.constants import EvalType, OutputType

SUBSET_LIST = [
    'computer science', 'math', 'chemistry', 'engineering', 'law', 'biology', 'health', 'physics', 'business',
    'philosophy', 'economics', 'other', 'psychology', 'history'
]  # è‡ªå®šä¹‰çš„å­æ•°æ®é›†åˆ—è¡¨

@Benchmark.register(
    name='mmlu_pro',
    pretty_name='MMLU-Pro',
    tags=['MCQ', 'Knowledge'],
    description=
    'MMLU-Pro is a benchmark for evaluating language models on multiple-choice questions across various subjects. It includes questions from different domains, where the model must select the correct answer from given options.',  # noqa: E501
    dataset_id='modelscope/MMLU-Pro',
    model_adapter=OutputType.GENERATION,
    output_types=[OutputType.MULTIPLE_CHOICE, OutputType.GENERATION],
    subset_list=SUBSET_LIST,
    metric_list=['AverageAccuracy'],
    few_shot_num=5,
    train_split='validation',
    eval_split='test',
    prompt_template=
    'The following are multiple choice questions (with answers) about {subset_name}. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n{query}',  # noqa: E501
)
class MMLUProAdapter(DataAdapter):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
```


## 3. ç¼–å†™è¯„æµ‹é€»è¾‘

åœ¨å®Œæˆ`Benchmark`æ³¨å†Œåï¼Œæ¥ä¸‹æ¥éœ€è¦ç¼–å†™`DataAdapter`ç±»ä¸­çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä»¥å®ç°è¯„æµ‹åŠŸèƒ½ã€‚è¿™äº›æ–¹æ³•æ§åˆ¶ç€æ•°æ®çš„åŠ è½½ã€å¤„ç†ä»¥åŠè¯„åˆ†æµç¨‹ã€‚

### è¯„æµ‹æµç¨‹æ¦‚è¿°

EvalScopeè¯„æµ‹æµç¨‹ä¸»è¦åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ•°æ®åŠ è½½**ï¼šé€šè¿‡`load`æ–¹æ³•åŠ è½½æ•°æ®é›†
2. **ç”Ÿæˆæç¤º**ï¼šé€šè¿‡`gen_prompts`è°ƒç”¨`gen_prompt`ç”Ÿæˆæ¨¡å‹è¾“å…¥
3. **æ¨¡å‹æ¨ç†**ï¼šé€šè¿‡æ¨¡å‹adapterçš„`predict`æ–¹æ³•å¯¹ç”Ÿæˆçš„æç¤ºè¿›è¡Œæ¨ç†
4. **ç­”æ¡ˆè§£æ**ï¼šé€šè¿‡`parse_pred_result`è§£ææ¨¡å‹è¾“å‡º
5. **ç­”æ¡ˆè¯„åˆ†**ï¼šé€šè¿‡`match`æˆ–`llm_match`æ–¹æ³•è¯„ä¼°é¢„æµ‹ç»“æœ
6. **æŒ‡æ ‡è®¡ç®—**ï¼šé€šè¿‡`compute_metric`è®¡ç®—è¯„ä¼°æŒ‡æ ‡
7. **æŠ¥å‘Šç”Ÿæˆ**ï¼šé€šè¿‡`gen_report`å’Œ`post_process_report`ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š


### å¿…é¡»å®ç°çš„æ ¸å¿ƒæ–¹æ³•

ä»¥ä¸‹æ˜¯å¿…é¡»å®ç°çš„æ ¸å¿ƒæ–¹æ³•ï¼Œæ¯ä¸ªæ–¹æ³•éƒ½æœ‰æ˜ç¡®çš„åŠŸèƒ½å’Œä½œç”¨ï¼š

1. **`gen_prompt`**ï¼šå°†æ•°æ®é›†æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„è¾“å…¥æ ¼å¼
   - å¤„ç†few-shotç¤ºä¾‹
   - æ ¼å¼åŒ–é—®é¢˜å’Œé€‰é¡¹
   - åº”ç”¨æç¤ºæ¨¡æ¿

2. **`get_gold_answer`**ï¼šä»æ•°æ®é›†æ ·æœ¬ä¸­æå–æ ‡å‡†ç­”æ¡ˆ
   - é€šå¸¸è¿”å›æ•°æ®é›†ä¸­çš„ç­”æ¡ˆå­—æ®µ

3. **`parse_pred_result`**ï¼šè§£ææ¨¡å‹è¾“å‡ºï¼Œæå–æœ‰æ•ˆç­”æ¡ˆ
   - å¯¹äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé€šå¸¸éœ€è¦æå–ç­”æ¡ˆé€‰é¡¹
   - å¯¹äºå¤šé€‰é¢˜ç›´æ¥è¾“å‡ºï¼Œå¯ä»¥ç›´æ¥è¿”å›ç»“æœ

4. **`match`**ï¼šæ¯”è¾ƒé¢„æµ‹ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆï¼Œè®¡ç®—å¾—åˆ†
   - é€šå¸¸ä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼ˆexact_matchï¼‰ç­‰æ–¹æ³•


### å¯é€‰å®ç°çš„æ–¹æ³•

é™¤äº†å¿…é¡»å®ç°çš„æ–¹æ³•å¤–ï¼Œè¿˜å¯ä»¥æ ¹æ®éœ€è¦å®ç°æˆ–é‡å†™ä»¥ä¸‹æ–¹æ³•ï¼š

1. **`llm_match`**ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤å™¨è¯„ä¼°ç­”æ¡ˆè´¨é‡
   - é€‚ç”¨äºå¼€æ”¾å¼é—®é¢˜æˆ–éœ€è¦å¤æ‚ç†è§£çš„è¯„æµ‹ä»»åŠ¡
   - éœ€è¦åœ¨é…ç½®ä¸­æŒ‡å®šè¯„åˆ¤æ¨¡å‹
   - ç›¸æ¯”ç®€å•è§„åˆ™åŒ¹é…ï¼Œèƒ½æ›´å¥½åœ°è¯„ä¼°ç­”æ¡ˆè´¨é‡

   ```python
   def llm_match(self, gold: Any, pred: Any, judge: Optional[LLMJudge] = None, **kwargs) -> float:
       """
       ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤å™¨è¯„ä¼°é¢„æµ‹ç­”æ¡ˆ
       
       Args:
           gold: æ ‡å‡†ç­”æ¡ˆ
           pred: é¢„æµ‹ç­”æ¡ˆ
           judge: LLMè¯„åˆ¤å™¨å®ä¾‹
           
       Returns:
           è¯„åˆ†ç»“æœï¼Œé€šå¸¸ä¸º0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°
       """
       # é»˜è®¤è¯„åˆ¤å™¨å¤„ç†
       if judge is None:
           return 0
           
       # æ„å»ºè¯„åˆ¤æç¤ºå¹¶è·å–è¯„åˆ†
       prompt = judge.build_prompt(pred, gold, kwargs.get('raw_input', {}).get('question'))
       score = judge(prompt)
       return judge.get_score(score)
   ```

2. **`post_process_report`**ï¼šå¤„ç†è¯„æµ‹æŠ¥å‘Šï¼Œæ·»åŠ è‡ªå®šä¹‰åˆ†ææˆ–å¯è§†åŒ–

3. **`load`**ï¼šé‡å†™æ•°æ®åŠ è½½æµç¨‹ï¼Œé€‚ç”¨äºéœ€è¦è‡ªå®šä¹‰æ•°æ®åŠ è½½é€»è¾‘çš„åœºæ™¯
   - é€‚åˆå¤„ç†ç‰¹æ®Šæ ¼å¼çš„æ•°æ®é›†
   - å¯ä»¥å®ç°è‡ªå®šä¹‰çš„å­é›†åˆ’åˆ†é€»è¾‘
   - å¯ä»¥æ·»åŠ æ•°æ®é¢„å¤„ç†æˆ–è¿‡æ»¤æ­¥éª¤

   ```python
   def load(self, dataset_name_or_path: str = None, subset_list: list = None, work_dir: Optional[str] = DEFAULT_DATASET_CACHE_DIR, **kwargs) -> dict:
       """
       è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½é€»è¾‘
       
       Args:
           dataset_name_or_path: æ•°æ®é›†è·¯å¾„æˆ–åç§°
           subset_list: å­é›†åˆ—è¡¨
           work_dir: å·¥ä½œç›®å½•
           
       Returns:
           æ•°æ®å­—å…¸ï¼Œæ ¼å¼ä¸º: {'subset_name': {'train': train_dataset, 'test': test_dataset}}
       """
       # å¯ä»¥åœ¨è¿™é‡Œå®ç°è‡ªå®šä¹‰çš„æ•°æ®åŠ è½½å’Œå¤„ç†é€»è¾‘
       # ä¾‹å¦‚ï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®ã€è¿‡æ»¤æ•°æ®ã€é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„ç­‰
       
       # è°ƒç”¨çˆ¶ç±»æ–¹æ³•åŠ è½½åŸºç¡€æ•°æ®
       data_dict = super().load(dataset_name_or_path, subset_list, work_dir, **kwargs)
       
       # è¿›è¡Œè‡ªå®šä¹‰å¤„ç†ï¼Œå¦‚æ ¹æ®ç‰¹å®šå­—æ®µè¿›è¡Œå­é›†åˆ’åˆ†
       return self.reformat_subset(data_dict, subset_key='your_category_field')
   ```

4. **`load_from_disk`**ï¼šä¸“é—¨ç”¨äºä»æœ¬åœ°ç£ç›˜åŠ è½½æ•°æ®é›†
   - å½“ä½¿ç”¨æœ¬åœ°æ•°æ®é›†è€ŒéModelScopeæ‰˜ç®¡æ•°æ®é›†æ—¶ï¼Œéœ€è¦é‡å†™æ­¤æ–¹æ³•
   - å¯ä»¥å¤„ç†è‡ªå®šä¹‰æ ¼å¼çš„æœ¬åœ°æ•°æ®æ–‡ä»¶

   ```python
   def load_from_disk(self, dataset_path, subset_list, work_dir, **kwargs) -> dict:
       """
       ä»æœ¬åœ°ç£ç›˜åŠ è½½æ•°æ®é›†
       
       Args:
           dataset_path: æœ¬åœ°æ•°æ®é›†è·¯å¾„
           subset_list: å­é›†åˆ—è¡¨
           work_dir: å·¥ä½œç›®å½•
           
       Returns:
           æ•°æ®å­—å…¸ï¼Œæ ¼å¼ä¸º: {'subset_name': {'train': train_dataset, 'test': test_dataset}}
       """
       # ç¤ºä¾‹ï¼šä»æœ¬åœ°JSONæ–‡ä»¶åŠ è½½æ•°æ®
       import json
       import os
       
       data_dict = {}
       for subset in subset_list:
           data_dict[subset] = {}
           
           # åŠ è½½è®­ç»ƒé›†ï¼ˆfew-shotç¤ºä¾‹ï¼‰
           if self.train_split:
               train_path = os.path.join(dataset_path, f"{subset}_{self.train_split}.json")
               if os.path.exists(train_path):
                   with open(train_path, 'r', encoding='utf-8') as f:
                       data_dict[subset][self.train_split] = json.load(f)
           
           # åŠ è½½æµ‹è¯•é›†
           if self.eval_split:
               test_path = os.path.join(dataset_path, f"{subset}_{self.eval_split}.json")
               if os.path.exists(test_path):
                   with open(test_path, 'r', encoding='utf-8') as f:
                       data_dict[subset][self.eval_split] = json.load(f)
       
       return data_dict
   ```

### ä»£ç ç¤ºä¾‹ä¸è§£é‡Š

ä¸‹é¢æ˜¯MMLU-Proé€‚é…å™¨çš„å®Œæ•´å®ç°ï¼ŒåŒ…å«è¯¦ç»†æ³¨é‡Šï¼š

```python
class MMLUProAdapter(DataAdapter):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # å®šä¹‰é€‰é¡¹æ ‡è¯†ç¬¦ï¼Œç”¨äºæ„å»ºé€‰é¡¹
        self.choices = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
    
    def load(self, **kwargs):
        """
        é‡å†™åŠ è½½æ–¹æ³•ï¼Œå®ç°è‡ªå®šä¹‰çš„æ•°æ®é›†åŠ è½½å’Œå­é›†åˆ’åˆ†é€»è¾‘
        
        åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®'category'å­—æ®µå¯¹æ•°æ®é›†è¿›è¡Œå­é›†åˆ’åˆ†
        """
        # å…ˆä½¿ç”¨é»˜è®¤æ–¹å¼åŠ è½½æ‰€æœ‰æ•°æ®
        kwargs['subset_list'] = ['default']
        data_dict = super().load(**kwargs)
        # ä½¿ç”¨'category'å­—æ®µä½œä¸ºå­é›†é”®å€¼è¿›è¡Œé‡æ–°æ ¼å¼åŒ–
        return self.reformat_subset(data_dict, subset_key='category')
    
    def gen_prompt(self, input_d: Dict, subset_name: str, few_shot_list: list, **kwargs) -> Any:
        """
        ç”Ÿæˆæ¨¡å‹è¾“å…¥çš„æç¤ºæ–‡æœ¬
        
        Args:
            input_d: å½“å‰æ ·æœ¬æ•°æ®
            subset_name: å­é›†åç§°ï¼Œç”¨äºæ¨¡æ¿å¡«å……
            few_shot_list: few-shotç¤ºä¾‹åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºæ–‡æœ¬
        """
        # å¤„ç†few-shotç¤ºä¾‹
        if self.few_shot_num > 0:
            prefix = self.format_fewshot_examples(few_shot_list)
        else:
            prefix = ''
        
        # æ„å»ºå½“å‰é—®é¢˜æ–‡æœ¬
        query = prefix + 'Q: ' + input_d['question'] + '\n' + \
            self.__form_options(input_d['options']) + '\n'

        # åº”ç”¨æç¤ºæ¨¡æ¿
        full_prompt = self.prompt_template.format(subset_name=subset_name, query=query)
        return self.gen_prompt_data(full_prompt)
    
    def format_fewshot_examples(self, few_shot_list):
        """
        æ ¼å¼åŒ–few-shotç¤ºä¾‹
        
        å°†æ¯ä¸ªç¤ºä¾‹æ ¼å¼åŒ–ä¸ºä¸€è‡´çš„æ ¼å¼ï¼ŒåŒ…æ‹¬é—®é¢˜ã€é€‰é¡¹å’Œæ€è€ƒè¿‡ç¨‹
        """
        prompts = ''
        for index, d in enumerate(few_shot_list):
            prompts += 'Q: ' + d['question'] + '\n' + \
                self.__form_options(d['options']) + '\n' + \
                d['cot_content'] + '\n\n'  # åŒ…å«æ€è€ƒè¿‡ç¨‹
        return prompts
    
    def __form_options(self, options: list):
        """
        æ ¼å¼åŒ–é€‰é¡¹åˆ—è¡¨
        
        å°†é€‰é¡¹æ•°ç»„è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„æ–‡æœ¬ï¼Œæ¯ä¸ªé€‰é¡¹å‰æ·»åŠ æ ‡è¯†ç¬¦(A)ã€(B)ç­‰
        """
        option_str = 'Options are:\n'
        for opt, choice in zip(options, self.choices):
            option_str += f'({choice}): {opt}' + '\n'
        return option_str
    
    def get_gold_answer(self, input_d: dict) -> str:
        """
        æå–æ ‡å‡†ç­”æ¡ˆ
        
        ä»æ•°æ®æ ·æœ¬ä¸­æå–æ­£ç¡®ç­”æ¡ˆï¼Œé€šå¸¸æ˜¯'A'ã€'B'ã€'C'ã€'D'ç­‰é€‰é¡¹æ ‡è¯†ç¬¦
        
        Args:
            input_d: è¾“å…¥æ•°æ®æ ·æœ¬
        
        Returns:
            æ ‡å‡†ç­”æ¡ˆå­—ç¬¦ä¸²
        """
        return input_d['answer']  # ç›´æ¥è¿”å›æ•°æ®é›†ä¸­çš„answerå­—æ®µ

    def parse_pred_result(self, result: str, raw_input_d: dict = None, eval_type: str = EvalType.CHECKPOINT) -> str:
        """
        è§£ææ¨¡å‹é¢„æµ‹ç»“æœ
        
        æ ¹æ®æ¨¡å‹ç±»å‹ä¸åŒï¼Œä½¿ç”¨ä¸åŒçš„è§£ææ–¹æ³•ï¼š
        - å¯¹äºç›´æ¥è¾“å‡ºé€‰é¡¹çš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›ç»“æœ
        - å¯¹äºç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹ï¼Œéœ€è¦ä»æ–‡æœ¬ä¸­æå–é€‰é¡¹
        
        Args:
            result: æ¨¡å‹é¢„æµ‹ç»“æœ
            raw_input_d: åŸå§‹è¾“å…¥æ•°æ®
            eval_type: è¯„æµ‹ç±»å‹
        
        Returns:
            è§£æåçš„ç­”æ¡ˆé€‰é¡¹
        """
        if self.model_adapter == OutputType.MULTIPLE_CHOICE:
            # å¤šé€‰é¢˜ç›´æ¥è¾“å‡ºæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç»“æœ
            return result
        else:
            # æ–‡æœ¬ç”Ÿæˆæ¨¡å¼ï¼Œä»æ–‡æœ¬ä¸­æå–é¦–ä¸ªé€‰é¡¹å­—æ¯
            return ResponseParser.parse_first_option(result)

    def match(self, gold: str, pred: str) -> float:
        """
        æ¯”è¾ƒé¢„æµ‹ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆ
        
        Args:
            gold: æ ‡å‡†ç­”æ¡ˆï¼Œå¦‚'A'
            pred: é¢„æµ‹ç­”æ¡ˆï¼Œå¦‚'B'
        
        Returns:
            åŒ¹é…å¾—åˆ†ï¼šæ­£ç¡®ä¸º1.0ï¼Œé”™è¯¯ä¸º0.0
        """
        return exact_match(gold=gold, pred=pred)  # ä½¿ç”¨ç²¾ç¡®åŒ¹é…
```

### æç¤ºå’Œæœ€ä½³å®è·µ

- åœ¨è®¾è®¡few-shotç¤ºä¾‹æ—¶ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´ä¸”åŒ…å«è¶³å¤Ÿä¿¡æ¯
- ä»”ç»†è®¾è®¡æç¤ºæ¨¡æ¿ï¼Œç¡®ä¿æ¨¡å‹èƒ½ç†è§£ä»»åŠ¡è¦æ±‚
- å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œè€ƒè™‘åœ¨`match`æ–¹æ³•ä¸­å®ç°æ›´çµæ´»çš„è¯„åˆ†é€»è¾‘
- æ·»åŠ è¶³å¤Ÿçš„æ³¨é‡Šå’Œæ–‡æ¡£ï¼Œæ–¹ä¾¿å…¶ä»–å¼€å‘è€…ç†è§£å’Œç»´æŠ¤ä»£ç 
- è€ƒè™‘ä½¿ç”¨`llm_match`æ–¹æ³•è¿›è¡Œæ›´å¤æ‚çš„ç­”æ¡ˆè¯„ä¼°ï¼Œå°¤å…¶æ˜¯å¯¹äºå¼€æ”¾å¼é—®é¢˜


## 4. è¿è¡Œè¯„æµ‹

è°ƒè¯•ä»£ç ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œã€‚

```python
from evalscope import run_task, TaskConfig
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct',
    datasets=['mmlu_pro'],
    limit=10,
    dataset_args={'mmlu_pro': {'subset_list': ['computer science', 'math']}},
    debug=True
)
run_task(task_cfg=task_cfg)
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```text
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+
| Model                 | Dataset   | Metric          | Subset           |   Num |   Score | Cat.0   |
+=======================+===========+=================+==================+=======+=========+=========+
| Qwen2.5-0.5B-Instruct | mmlu_pro  | AverageAccuracy | computer science |     10 |       0.1 | default |
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+
| Qwen2.5-0.5B-Instruct | mmlu_pro  | AverageAccuracy | math             |     10 |       0.1 | default |
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+ 
```

## 5. åŸºå‡†è¯„æµ‹æ–‡æ¡£ç”Ÿæˆ

å®ŒæˆåŸºå‡†è¯„æµ‹å®ç°åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨EvalScopeæä¾›çš„å·¥å…·ç”Ÿæˆæ ‡å‡†æ–‡æ¡£ã€‚è¿™å°†ç¡®ä¿æ‚¨çš„åŸºå‡†è¯„æµ‹æœ‰ä¸€è‡´çš„æ–‡æ¡£æ ¼å¼ï¼Œå¹¶èƒ½å¤Ÿè¢«å…¶ä»–ç”¨æˆ·è½»æ¾ç†è§£å’Œä½¿ç”¨ã€‚

è¦ç”Ÿæˆä¸­è‹±æ–‡æ–‡æ¡£ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå°†æ ¹æ®æ³¨å†Œä¿¡æ¯ç”Ÿæˆæ–‡æ¡£ï¼š

```bash
# è¿›å…¥evalscopeæ ¹ç›®å½•
cd /path/to/evalscope

# ç”ŸæˆåŸºå‡†è¯„æµ‹æ–‡æ¡£
python docs/generate_dataset_md.py
```

å®Œæˆè¿™äº›æ–¹æ³•çš„å®ç°å’Œæ–‡æ¡£ç”Ÿæˆåï¼Œæ‚¨çš„åŸºå‡†è¯„æµ‹å°±å‡†å¤‡å°±ç»ªäº†ï¼å¯ä»¥æäº¤[PR](https://github.com/modelscope/evalscope/pulls)äº†ï¼Œæˆ‘ä»¬å°†å°½å¿«åˆå¹¶ä½ çš„è´¡çŒ®ï¼Œè®©æ›´å¤šç”¨æˆ·æ¥ä½¿ç”¨ä½ è´¡çŒ®çš„åŸºå‡†è¯„æµ‹ã€‚å¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•æäº¤PRï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„[æŒ‡å—](https://github.com/modelscope/evalscope/blob/main/CONTRIBUTING.md)ï¼Œå¿«æ¥è¯•ä¸€è¯•å§ğŸš€
