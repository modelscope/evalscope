# ğŸ‘ è´¡çŒ®åŸºå‡†è¯„æµ‹

EvalScopeä½œä¸º[ModelScope](https://modelscope.cn)çš„å®˜æ–¹è¯„æµ‹å·¥å…·ï¼Œå…¶åŸºå‡†è¯„æµ‹åŠŸèƒ½æ­£åœ¨æŒç»­ä¼˜åŒ–ä¸­ï¼æˆ‘ä»¬è¯šé‚€æ‚¨å‚è€ƒæœ¬æ•™ç¨‹ï¼Œè½»æ¾æ·»åŠ è‡ªå·±çš„è¯„æµ‹åŸºå‡†ï¼Œå¹¶ä¸å¹¿å¤§ç¤¾åŒºæˆå‘˜åˆ†äº«æ‚¨çš„è´¡çŒ®ã€‚ä¸€èµ·åŠ©åŠ›EvalScopeçš„æˆé•¿ï¼Œè®©æˆ‘ä»¬çš„å·¥å…·æ›´åŠ å‡ºè‰²ï¼

ä¸‹é¢å°†ä»‹ç»å¦‚ä½•æ·»åŠ **é€šç”¨æ–‡æœ¬æ¨ç†**å’Œ**å¤šé¡¹é€‰æ‹©**ä¸¤ç§åŸºå‡†è¯„æµ‹ï¼Œä¸»è¦åŒ…å«ä¸Šä¼ æ•°æ®é›†ã€æ³¨å†Œæ•°æ®é›†ã€ç¼–å†™è¯„æµ‹ä»»åŠ¡ä¸‰ä¸ªæ­¥éª¤ã€‚

## åŸºç¡€æ¦‚å¿µ

```{tip}
æ‚¨å¯ä»¥å…ˆè·³è¿‡æœ¬èŠ‚ï¼Œç›´æ¥ä»[å‡†å¤‡åŸºå‡†è¯„æµ‹æ•°æ®é›†](#1-å‡†å¤‡åŸºå‡†è¯„æµ‹æ•°æ®é›†)å¼€å§‹ï¼Œé‡åˆ°ä¸ç†è§£çš„ä»£ç åå†æŸ¥çœ‹å…·ä½“çš„å®ç°ã€‚
```

EvalScopeè¯„æµ‹æµç¨‹ä¸»è¦åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
1. **æ•°æ®å‡†å¤‡**ï¼šé€šè¿‡`DataAdapter`åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ã€‚
2. **ä»»åŠ¡å®šä¹‰**ï¼šé€šè¿‡`TaskConfig`å®šä¹‰è¯„æµ‹ä»»åŠ¡çš„é…ç½®ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ç­‰ã€‚
3. **è¯„æµ‹æ‰§è¡Œ**ï¼šé€šè¿‡`run_task`å‡½æ•°æ‰§è¡Œè¯„æµ‹ä»»åŠ¡ï¼Œå¹¶è¾“å‡ºè¯„æµ‹ç»“æœã€‚

å…¶ä¸­`DataAdapter`æ˜¯æˆ‘ä»¬éœ€è¦é‡ç‚¹äº†è§£çš„ç±»ï¼Œå®ƒæ˜¯åŸºå‡†è¯„æµ‹çš„æ ¸å¿ƒç»„ä»¶ã€‚

### DataAdapteræ¶æ„å’Œè°ƒç”¨æµç¨‹

DataAdapteré‡‡ç”¨Pipelineæ¶æ„ï¼Œæ”¯æŒé€šè¿‡é’©å­æ–¹æ³•è‡ªå®šä¹‰è¡Œä¸ºã€‚ä»¥`DefaultDataAdapter`ä¸ºä¾‹ï¼Œå®Œæ•´çš„è¯„æµ‹æµç¨‹å¦‚ä¸‹ï¼š

```
1. æ•°æ®åŠ è½½é˜¶æ®µ
   load_dataset() 
   â”œâ”€â”€ load() 
   â”‚   â”œâ”€â”€ load_from_remote() / load_from_disk()
   â”‚   â”‚   â”œâ”€â”€ load_subsets()
   â”‚   â”‚   â”‚   â””â”€â”€ load_subset() / load_fewshot_subset()
   â”‚   â”‚   â”‚       â””â”€â”€ record_to_sample() [ç”¨æˆ·å®ç°]
   â”‚   â”‚   â””â”€â”€ _post_process_samples()
   â”‚   â”‚       â””â”€â”€ process_sample_input()
   â”‚   â”‚           â”œâ”€â”€ sample_to_fewshot() [ç”¨æˆ·å®ç°]
   â”‚   â”‚           â”œâ”€â”€ format_fewshot_template() [ç”¨æˆ·å¯é€‰å®ç°]
   â”‚   â”‚           â””â”€â”€ format_prompt_template() [ç”¨æˆ·å¯é€‰å®ç°]
   â”‚   â””â”€â”€ è¿”å› DatasetDict

2. æ¨¡å‹æ¨ç†é˜¶æ®µï¼ˆæ¯ä¸ªæ ·æœ¬ï¼‰
   run_inference()
   â”œâ”€â”€ _on_inference_start() [é’©å­æ–¹æ³•]
   â”œâ”€â”€ _on_inference() [é’©å­æ–¹æ³•]
   â””â”€â”€ _on_inference_end() [é’©å­æ–¹æ³•]
       â””â”€â”€ è¿”å› TaskState

3. æŒ‡æ ‡è®¡ç®—é˜¶æ®µï¼ˆæ¯ä¸ªæ ·æœ¬ï¼‰
   calculate_metrics()
   â”œâ”€â”€ filter_prediction()
   â”‚   â””â”€â”€ extract_answer() [ç”¨æˆ·å¯é€‰å®ç°]
   â”œâ”€â”€ match_score() / llm_match_score()
   â””â”€â”€ è¿”å› SampleScore

4. ç»“æœèšåˆé˜¶æ®µ
   aggregate_scores()
   â””â”€â”€ è¿”å› List[AggScore]

5. æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ
   generate_report()
   â”œâ”€â”€ _on_generate_report() [é’©å­æ–¹æ³•]
   â””â”€â”€ _on_generate_report_end() [é’©å­æ–¹æ³•]
       â””â”€â”€ è¿”å› Report
```

### æ ¸å¿ƒæ•°æ®ç»“æ„

#### 1. Sampleå¯¹è±¡
è¡¨ç¤ºå•ä¸ªè¯„æµ‹æ ·æœ¬ï¼ŒåŒ…å«è¾“å…¥ã€ç›®æ ‡ç­”æ¡ˆå’Œå…ƒæ•°æ®ï¼š

```python
@dataclass
class Sample:
    input: Any                    # è¾“å…¥å†…å®¹ï¼ˆé—®é¢˜æ–‡æœ¬æˆ–èŠå¤©æ¶ˆæ¯åˆ—è¡¨ï¼‰
    target: str                   # ç›®æ ‡ç­”æ¡ˆï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰
    choices: Optional[List[str]] = None    # é€‰æ‹©é¡¹ï¼ˆå¤šé€‰é¢˜ä½¿ç”¨ï¼‰
    subset_key: Optional[str] = None       # å­é›†åˆ’åˆ†é”®ï¼ˆç”¨äºæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰
    metadata: Optional[Dict] = None        # å…ƒæ•°æ®ï¼ˆæ¨ç†è¿‡ç¨‹ã€IDç­‰ï¼‰
    tools: Optional[List] = None           # å·¥å…·è°ƒç”¨ä¿¡æ¯
```

#### 2. TaskStateå¯¹è±¡
è¡¨ç¤ºå•æ¬¡æ¨ç†ä»»åŠ¡çš„å®Œæ•´çŠ¶æ€ï¼š

```python
@dataclass
class TaskState:
    model: str                    # æ¨¡å‹åç§°
    sample: Sample               # è¾“å…¥æ ·æœ¬
    messages: List[ChatMessage]  # èŠå¤©æ¶ˆæ¯å†å²
    output: ModelOutput          # æ¨¡å‹åŸå§‹è¾“å‡º
    completed: bool              # ä»»åŠ¡æ˜¯å¦å®Œæˆ
    sample_id: Optional[str] = None      # æ ·æœ¬ID
    group_id: Optional[str] = None       # åˆ†ç»„ID
    metadata: Optional[Dict] = None      # ä»»åŠ¡å…ƒæ•°æ®
```

#### 3. ModelOutputå¯¹è±¡
è¡¨ç¤ºæ¨¡å‹çš„åŸå§‹è¾“å‡ºï¼š

```python
@dataclass
class ModelOutput:
    completion: str              # æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
    message: ChatMessage         # æ ¼å¼åŒ–çš„èŠå¤©æ¶ˆæ¯
    # å…¶ä»–æ¨¡å‹ç‰¹å®šå­—æ®µ...
```

#### 4. Scoreå¯¹è±¡
è¡¨ç¤ºå•ä¸ªæ ·æœ¬çš„è¯„åˆ†ç»“æœï¼š

```python
@dataclass
class Score:
    value: Dict[str, float]      # å„æŒ‡æ ‡çš„å¾—åˆ† {"acc": 1.0, "f1": 0.8}
    extracted_prediction: str    # æå–çš„é¢„æµ‹ç­”æ¡ˆ
    prediction: str              # åŸå§‹é¢„æµ‹æ–‡æœ¬
    metadata: Dict = None        # è¯„åˆ†å…ƒæ•°æ®
```

#### 5. SampleScoreå¯¹è±¡
å°è£…å•ä¸ªæ ·æœ¬çš„å®Œæ•´è¯„åˆ†ä¿¡æ¯ï¼š

```python
@dataclass
class SampleScore:
    score: Score                 # è¯„åˆ†å¯¹è±¡
    sample_id: Optional[str]     # æ ·æœ¬å”¯ä¸€æ ‡è¯†
    group_id: Optional[str]      # åˆ†ç»„æ ‡è¯†
    sample_metadata: Optional[Dict] = None  # æ ·æœ¬å…ƒæ•°æ®
```

#### 6. AggScoreå¯¹è±¡
è¡¨ç¤ºèšåˆåçš„è¯„åˆ†ç»Ÿè®¡ï¼š

```python
@dataclass
class AggScore:
    metric: str                  # æŒ‡æ ‡åç§°
    value: float                 # èšåˆå€¼ï¼ˆå¦‚å¹³å‡åˆ†ï¼‰
    subset: str                  # å­é›†åç§°
    num_samples: int             # æ ·æœ¬æ•°é‡
    agg_method: str              # èšåˆæ–¹æ³•ï¼ˆmean, medianç­‰ï¼‰
    metadata: Dict = None        # èšåˆå…ƒæ•°æ®
```

#### 7. DatasetDictå¯¹è±¡
ç®¡ç†å¤šä¸ªæ•°æ®é›†å­é›†ï¼š

```python
class DatasetDict(dict):
    """æ•°æ®é›†å­—å…¸ï¼Œé”®ä¸ºå­é›†åç§°ï¼Œå€¼ä¸ºDatasetå¯¹è±¡"""
    
    @classmethod
    def from_dataset(cls, dataset, subset_list=None, limit=None, repeats=1):
        """ä»å•ä¸ªæ•°æ®é›†åˆ›å»ºå¤šå­é›†æ•°æ®é›†å­—å…¸"""
        pass
```

### DataAdapteræ ¸å¿ƒæ–¹æ³•è¯¦è§£

åŸºäºä¸Šè¿°è°ƒç”¨æµç¨‹ï¼Œä»¥ä¸‹æ˜¯éœ€è¦ç”¨æˆ·å®ç°æˆ–å¯é€‰é‡å†™çš„å…³é”®æ–¹æ³•ï¼š

#### å¿…é¡»å®ç°çš„æ–¹æ³•

1. **`record_to_sample(record: Dict[str, Any]) -> Sample`**
   - **ä½œç”¨**ï¼šå°†åŸå§‹æ•°æ®è®°å½•è½¬æ¢ä¸ºæ ‡å‡†Sampleå¯¹è±¡
   - **è¾“å…¥**ï¼šæ•°æ®é›†ä¸­çš„åŸå§‹è®°å½•å­—å…¸
   - **è¾“å‡º**ï¼šæ ‡å‡†åŒ–çš„Sampleå¯¹è±¡
   - **ç¤ºä¾‹**ï¼š
   ```python
   def record_to_sample(self, record: Dict[str, Any]) -> Sample:
       return Sample(
           input=record['question'],
           target=record['answer'],
           metadata={'reasoning': record.get('explanation', '')}
       )
   ```

#### å¯é€‰å®ç°çš„æ–¹æ³•

2. **`sample_to_fewshot(sample: Sample) -> str`**
   - **ä½œç”¨**ï¼šå°†æ ·æœ¬è½¬æ¢ä¸ºfew-shotç¤ºä¾‹å­—ç¬¦ä¸²
   - **è¾“å…¥**ï¼šSampleå¯¹è±¡
   - **è¾“å‡º**ï¼šæ ¼å¼åŒ–çš„few-shotç¤ºä¾‹æ–‡æœ¬
   - **è°ƒç”¨æ—¶æœº**ï¼šæ„å»ºfew-shotæç¤ºæ—¶

3. **`extract_answer(prediction: str, task_state: TaskState) -> str`**
   - **ä½œç”¨**ï¼šä»æ¨¡å‹åŸå§‹è¾“å‡ºä¸­æå–æœ€ç»ˆç­”æ¡ˆ
   - **è¾“å…¥**ï¼šæ¨¡å‹é¢„æµ‹æ–‡æœ¬å’Œä»»åŠ¡çŠ¶æ€
   - **è¾“å‡º**ï¼šæå–çš„ç­”æ¡ˆå­—ç¬¦ä¸²
   - **è°ƒç”¨æ—¶æœº**ï¼šè®¡ç®—æŒ‡æ ‡å‰çš„ç­”æ¡ˆæ¸…ç†

4. **`format_prompt_template(sample: Sample) -> str`**
   - **ä½œç”¨**ï¼šæ ¼å¼åŒ–åŸºç¡€æç¤ºæ¨¡æ¿
   - **è¾“å…¥**ï¼šSampleå¯¹è±¡
   - **è¾“å‡º**ï¼šæ ¼å¼åŒ–çš„æç¤ºæ–‡æœ¬
   - **é»˜è®¤å®ç°**ï¼šä½¿ç”¨`prompt_template.format(question=sample.input)`

5. **`format_fewshot_template(fewshot: str, sample: Sample) -> str`**
   - **ä½œç”¨**ï¼šæ ¼å¼åŒ–åŒ…å«few-shotçš„æç¤ºæ¨¡æ¿
   - **è¾“å…¥**ï¼šfew-shotç¤ºä¾‹å­—ç¬¦ä¸²å’ŒSampleå¯¹è±¡
   - **è¾“å‡º**ï¼šå®Œæ•´çš„few-shotæç¤º
   - **é»˜è®¤å®ç°**ï¼šä½¿ç”¨`few_shot_prompt_template.format()`

6. **`sample_filter(sample: Sample) -> bool`**
   - **ä½œç”¨**ï¼šè¿‡æ»¤æ•°æ®é›†æ ·æœ¬
   - **è¾“å…¥**ï¼šSampleå¯¹è±¡
   - **è¾“å‡º**ï¼šæ˜¯å¦ä¿ç•™è¯¥æ ·æœ¬
   - **é»˜è®¤å®ç°**ï¼šè¿”å›Trueï¼ˆä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼‰

### é’©å­æ–¹æ³•ç³»ç»Ÿ

DataAdapteræä¾›äº†é’©å­æ–¹æ³•ç³»ç»Ÿï¼Œæ”¯æŒåœ¨å…³é”®èŠ‚ç‚¹æ’å…¥è‡ªå®šä¹‰é€»è¾‘ï¼š

#### æ¨ç†é˜¶æ®µé’©å­
- **`_on_inference_start(model, sample)`**ï¼šæ¨ç†å¼€å§‹å‰
- **`_on_inference(model, sample)`**ï¼šæ‰§è¡Œæ¨ç†
- **`_on_inference_end(model, sample, model_output, output_dir)`**ï¼šæ¨ç†ç»“æŸå

#### æŠ¥å‘Šç”Ÿæˆé’©å­
- **`_on_generate_report(scores, model_name)`**ï¼šç”ŸæˆæŠ¥å‘Š
- **`_on_generate_report_end(report, output_dir)`**ï¼šæŠ¥å‘Šç”Ÿæˆå

### é€‚é…å™¨ç±»å‹

EvalScopeæä¾›äº†ä¸¤ç§ä¸»è¦çš„é€‚é…å™¨åŸºç±»ï¼š

1. **`DefaultDataAdapter`**ï¼šé€šç”¨æ–‡æœ¬æ¨ç†ä»»åŠ¡çš„åŸºç¡€é€‚é…å™¨
   - é€‚ç”¨äºå¼€æ”¾å¼é—®ç­”ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡
   - éœ€è¦è‡ªå®šä¹‰ç­”æ¡ˆæå–é€»è¾‘

2. **`MultiChoiceAdapter`**ï¼šå¤šé¡¹é€‰æ‹©ä»»åŠ¡çš„ä¸“ç”¨é€‚é…å™¨
   - ç»§æ‰¿è‡ª`DefaultDataAdapter`
   - å†…ç½®é€‰æ‹©é¡¹æ ¼å¼åŒ–å’Œç­”æ¡ˆæå–é€»è¾‘
   - æ”¯æŒå•é€‰å’Œå¤šé€‰æ¨¡å¼

é€‰æ‹©é€‚é…å™¨ç±»å‹çš„åŸåˆ™ï¼š
- å¦‚æœä»»åŠ¡æ¶‰åŠä»å›ºå®šé€‰é¡¹ä¸­é€‰æ‹©ç­”æ¡ˆ â†’ ä½¿ç”¨`MultiChoiceAdapter`
- å¦‚æœä»»åŠ¡éœ€è¦ç”Ÿæˆå¼€æ”¾å¼ç­”æ¡ˆ â†’ ä½¿ç”¨`DefaultDataAdapter`

## 1. å‡†å¤‡åŸºå‡†è¯„æµ‹æ•°æ®é›†

æ‚¨æœ‰ä¸¤ç§æ–¹å¼å‡†å¤‡åŸºå‡†è¯„æµ‹æ•°æ®é›†ï¼š

1. **ä¸Šä¼ åˆ°ModelScopeï¼ˆæ¨èï¼‰**ï¼šå°†æ•°æ®é›†ä¸Šä¼ åˆ°ModelScopeå¹³å°ï¼Œè¿™æ ·å…¶ä»–ç”¨æˆ·å¯ä»¥ä¸€é”®åŠ è½½æ‚¨çš„æ•°æ®é›†ï¼Œä½¿ç”¨æ›´åŠ ä¾¿æ·ï¼Œä¹Ÿèƒ½è®©æ›´å¤šç”¨æˆ·å—ç›Šäºæ‚¨çš„è´¡çŒ®ã€‚å¦‚éœ€ä¸Šä¼ åˆ°ModelScopeï¼Œå¯å‚è€ƒ[æ•°æ®é›†ä¸Šä¼ æ•™ç¨‹](https://www.modelscope.cn/docs/datasets/create)ã€‚

2. **æœ¬åœ°ä½¿ç”¨**ï¼šæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨æœ¬åœ°æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼Œé€‚åˆæ•°æ®é›†å°šåœ¨å¼€å‘é˜¶æ®µæˆ–å«æœ‰æ•æ„Ÿä¿¡æ¯çš„æƒ…å†µã€‚

æ— è®ºé€‰æ‹©å“ªç§æ–¹å¼ï¼Œè¯·ç¡®ä¿æ•°æ®çš„æ ¼å¼æ­£ç¡®ä¸”å¯è¢«åŠ è½½ã€‚å¦‚ä½¿ç”¨æœ¬åœ°æ•°æ®é›†ï¼Œå¯é€šè¿‡ä»¥ä¸‹ä»£ç æµ‹è¯•ï¼š

```python
from modelscope import MsDataset

dataset = MsDataset.load("/path/to/your/dataset")  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†
```

## 2. åˆ›å»ºæ–‡ä»¶ç»“æ„

é¦–å…ˆ[Fork EvalScope](https://github.com/modelscope/evalscope/fork) ä»“åº“ï¼Œå³åˆ›å»ºä¸€ä¸ªè‡ªå·±çš„EvalScopeä»“åº“å‰¯æœ¬ï¼Œå°†å…¶cloneåˆ°æœ¬åœ°ã€‚

```bash
git clone https://github.com/your_username/evalscope.git
cd evalscope
```

ç„¶åï¼Œåœ¨`evalscope/benchmarks/`ç›®å½•ä¸‹æ·»åŠ åŸºå‡†è¯„æµ‹ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
evalscope/benchmarks/
â”œâ”€â”€ benchmark_name
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ benchmark_name_adapter.py
â”‚   â””â”€â”€ ...
```
å…·ä½“åˆ°`GSM8K`å’Œ`MMLU-Pro`ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
evalscope/benchmarks/
â”œâ”€â”€ gsm8k
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gsm8k_adapter.py
â”œâ”€â”€ mmlu_pro
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mmlu_pro_adapter.py
â”‚   â””â”€â”€ ...
```

## 3. ç¼–å†™è¯„æµ‹é€»è¾‘

ä¸‹é¢å°†ä»¥**GSM8K**å’Œ**MMLU-Pro**ä¸ºä¾‹ï¼Œåˆ†åˆ«ä»‹ç»**é€šç”¨æ–‡æœ¬æ¨ç†**å’Œ**å¤šé¡¹é€‰æ‹©**ä¸¤ç§è¯„æµ‹ä»»åŠ¡ã€‚

### é€šç”¨æ–‡æœ¬æ¨ç†

é€šç”¨æ–‡æœ¬æ¨ç†ä»»åŠ¡é€šå¸¸è¦æ±‚æ¨¡å‹å¯¹ç»™å®šé—®é¢˜è¿›è¡Œåˆ†æå’Œæ¨ç†ï¼Œç„¶åç”Ÿæˆç­”æ¡ˆã€‚ä»¥GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰ä¸ºä¾‹ï¼š

æˆ‘ä»¬éœ€è¦åœ¨`gsm8k_adapter.py`ä¸­æ³¨å†Œ`Benchmark`å¹¶å®ç°`GSM8KAdapter`ç±»ï¼š

```python
from typing import Any, Dict
from evalscope.api.benchmark import BenchmarkMeta, DefaultDataAdapter
from evalscope.api.dataset import Sample
from evalscope.api.evaluator import TaskState
from evalscope.api.registry import register_benchmark
from evalscope.constants import Tags

# å®šä¹‰æç¤ºæ¨¡æ¿
PROMPT_TEMPLATE = """
Solve the following math problem step by step. The last line of your response should be of the form "ANSWER: $ANSWER" (without quotes) where $ANSWER is the answer to the problem.

{question}

Remember to put your answer on its own line at the end in the form "ANSWER: $ANSWER" (without quotes) where $ANSWER is the answer to the problem, and you do not need to use a \\boxed command.

Reasoning:
""".lstrip()

# æ³¨å†ŒåŸºå‡†è¯„æµ‹
@register_benchmark(
    BenchmarkMeta(
        name='gsm8k',                          # åŸºå‡†æµ‹è¯•åç§°
        pretty_name='GSM8K',                   # å¯è¯»åç§°
        dataset_id='AI-ModelScope/gsm8k',      # æ•°æ®é›†ID æˆ– æœ¬åœ°è·¯å¾„
        tags=[Tags.MATH, Tags.REASONING],      # æ ‡ç­¾
        description='GSM8K (Grade School Math 8K) is a dataset of grade school math problems, designed to evaluate the mathematical reasoning abilities of AI models.',
        subset_list=['main'],                  # å­æ•°æ®é›†åˆ—è¡¨
        few_shot_num=4,                       # few-shotç¤ºä¾‹æ•°é‡
        train_split='train',                  # è®­ç»ƒé›†splitåç§°
        eval_split='test',                    # è¯„æµ‹é›†splitåç§°
        metric_list=['acc'],                  # è¯„ä¼°æŒ‡æ ‡
        prompt_template=PROMPT_TEMPLATE,      # æç¤ºæ¨¡æ¿
    )
)
class GSM8KAdapter(DefaultDataAdapter):
    
    def record_to_sample(self, record: Dict[str, Any]) -> Sample:
        """å°†åŸå§‹æ•°æ®è®°å½•è½¬æ¢ä¸ºSampleå¯¹è±¡"""
        DELIM = '####'
        question = record['question']
        answer = record['answer'].split(DELIM)
        target = answer.pop().strip()  # æå–æœ€ç»ˆç­”æ¡ˆ
        reasoning = DELIM.join(answer)  # æå–æ¨ç†è¿‡ç¨‹
        
        return Sample(
            input=question,
            target=target,
            metadata={'reasoning': reasoning.strip()}
        )
    
    def sample_to_fewshot(self, sample: Sample) -> str:
        """å°†æ ·æœ¬è½¬æ¢ä¸ºfew-shotç¤ºä¾‹"""
        if sample.metadata:
            return (
                f'{sample.input}\n\nReasoning:\n' + 
                f"{sample.metadata['reasoning']}\n\n" + 
                f'ANSWER: {sample.target}'
            )
        else:
            return ''
    
    def extract_answer(self, prediction: str, task_state: TaskState):
        """ä»æ¨¡å‹é¢„æµ‹ä¸­æå–ç­”æ¡ˆ"""
        from evalscope.filters.extraction import RegexFilter
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°å­—ç­”æ¡ˆ
        regex = RegexFilter(regex_pattern=r'(-?[0-9.,]{2,})|(-?[0-9]+)', group_select=-1)
        res = regex(prediction)
        return res.replace(',', '').replace('+', '').strip().strip('.')
```

### å¤šé¡¹é€‰æ‹©

å¤šé¡¹é€‰æ‹©ä»»åŠ¡è¦æ±‚æ¨¡å‹ä»ç»™å®šé€‰é¡¹ä¸­é€‰æ‹©æ­£ç¡®ç­”æ¡ˆã€‚ä»¥MMLU-Proä¸ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦ç»§æ‰¿`MultiChoiceAdapter`ï¼š

```python
from typing import Any, Dict
from evalscope.api.benchmark import BenchmarkMeta, MultiChoiceAdapter
from evalscope.api.dataset import Sample
from evalscope.api.registry import register_benchmark
from evalscope.constants import Tags

# å®šä¹‰æç¤ºæ¨¡æ¿
USER_PROMPT_TEMPLATE = """Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of {letters}. Think step by step before answering.

Question:
{question}
Options:
{choices}
""".lstrip()

SUBSET_LIST = [
    'computer science', 'math', 'chemistry', 'engineering', 'law', 'biology', 
    'health', 'physics', 'business', 'philosophy', 'economics', 'other', 
    'psychology', 'history'
]

@register_benchmark(
    BenchmarkMeta(
        name='mmlu_pro',
        pretty_name='MMLU-Pro',
        tags=[Tags.MULTIPLE_CHOICE, Tags.KNOWLEDGE],
        description='MMLU-Pro is a benchmark for evaluating language models on multiple-choice questions across various subjects.',
        dataset_id='modelscope/MMLU-Pro',
        subset_list=SUBSET_LIST,
        metric_list=['acc'],
        few_shot_num=5,
        train_split='validation',
        eval_split='test',
        prompt_template=USER_PROMPT_TEMPLATE,
    )
)
class MMLUProAdapter(MultiChoiceAdapter):
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.reformat_subset = True  # å¯ç”¨å­é›†åˆ’åˆ†
    
    def record_to_sample(self, record: Dict[str, Any]) -> Sample:
        """å°†åŸå§‹æ•°æ®è®°å½•è½¬æ¢ä¸ºSampleå¯¹è±¡"""
        return Sample(
            input=record['question'],
            choices=record['options'],      # é€‰æ‹©é¡¹åˆ—è¡¨
            target=record['answer'],        # æ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚'A'ï¼‰
            subset_key=record['category'].lower(),  # ç”¨äºå­é›†åˆ’åˆ†çš„key
            metadata={
                'cot_content': record['cot_content'],
                'subject': record['category'].lower(),
                'question_id': record['question_id'],
            },
        )
    
    def sample_to_fewshot(self, sample: Sample) -> str:
        """å°†æ ·æœ¬è½¬æ¢ä¸ºfew-shotç¤ºä¾‹"""
        q_str = f"""Question:\n{str(sample.input)}"""
        options = sample.choices if sample.choices is not None else []
        
        # æ ¼å¼åŒ–é€‰æ‹©é¡¹
        opt_str_list = []
        for i, opt in enumerate(options):
            opt_str_list.append(f"""{chr(65 + i)} {opt}""")
        opt_str = f"""Options:\n{'\n'.join(opt_str_list)}"""
        
        # å¤„ç†ç­”æ¡ˆå’Œæ¨ç†è¿‡ç¨‹
        ans_str = sample.metadata['cot_content'] if sample.metadata is not None else ''
        ans_str = ans_str.replace('The answer is', 'ANSWER:')
        ans_opt = ans_str.split('ANSWER:')[-1].split('.')[0].strip().strip('(').strip(')')
        ans_str = ans_str.replace(f'ANSWER: ({ans_opt})', f'ANSWER: {ans_opt}')
        
        final_str = '\n'.join([q_str, opt_str, ans_str])
        return final_str
```

### å…³é”®å·®å¼‚è¯´æ˜

**é€šç”¨æ–‡æœ¬æ¨ç†** vs **å¤šé¡¹é€‰æ‹©**ï¼š

1. **ç»§æ‰¿çš„åŸºç±»**ï¼š
   - é€šç”¨æ–‡æœ¬æ¨ç†ï¼šç»§æ‰¿`DefaultDataAdapter`
   - å¤šé¡¹é€‰æ‹©ï¼šç»§æ‰¿`MultiChoiceAdapter`

2. **Sampleå¯¹è±¡ç»“æ„**ï¼š
   - é€šç”¨æ–‡æœ¬æ¨ç†ï¼šä¸»è¦åŒ…å«`input`å’Œ`target`
   - å¤šé¡¹é€‰æ‹©ï¼šé¢å¤–åŒ…å«`choices`ï¼ˆé€‰æ‹©é¡¹åˆ—è¡¨ï¼‰

3. **ç­”æ¡ˆæå–æ–¹æ³•**ï¼š
   - é€šç”¨æ–‡æœ¬æ¨ç†ï¼šéœ€è¦è‡ªå®šä¹‰`extract_answer()`æ–¹æ³•
   - å¤šé¡¹é€‰æ‹©ï¼š`MultiChoiceAdapter`æä¾›äº†æ ‡å‡†çš„ç­”æ¡ˆæå–é€»è¾‘

4. **æç¤ºæ¨¡æ¿**ï¼š
   - é€šç”¨æ–‡æœ¬æ¨ç†ï¼šæ›´æ³¨é‡æ¨ç†è¿‡ç¨‹çš„å¼•å¯¼
   - å¤šé¡¹é€‰æ‹©ï¼šä¸“æ³¨äºé€‰æ‹©é¡¹çš„å±•ç¤ºå’Œç­”æ¡ˆæ ¼å¼

## 4. è¿è¡Œè¯„æµ‹

è°ƒè¯•ä»£ç ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œã€‚

**GSM8Kç¤ºä¾‹**ï¼š
```python
from evalscope import run_task, TaskConfig

task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct',
    datasets=['gsm8k'],
    limit=10,
    debug=True
)
run_task(task_cfg=task_cfg)
```

**MMLU-Proç¤ºä¾‹**ï¼š
```python
from evalscope import run_task, TaskConfig

task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct',
    datasets=['mmlu_pro'],
    limit=10,
    dataset_args={'mmlu_pro': {'subset_list': ['computer science', 'math']}},
    debug=True
)
run_task(task_cfg=task_cfg)
```

è¾“å‡ºç¤ºä¾‹ï¼š

```text
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+
| Model                 | Dataset   | Metric          | Subset           |   Num |   Score | Cat.0   |
+=======================+===========+=================+==================+=======+=========+=========+
| Qwen2.5-0.5B-Instruct | gsm8k     | mean_acc        | main             |    10 |     0.3 | default |
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+
| Qwen2.5-0.5B-Instruct | mmlu_pro  | mean_acc        | computer science |    10 |     0.1 | default |
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+
| Qwen2.5-0.5B-Instruct | mmlu_pro  | mean_acc        | math             |    10 |     0.1 | default |
+-----------------------+-----------+-----------------+------------------+-------+---------+---------+
```

## 5. åŸºå‡†è¯„æµ‹æ–‡æ¡£ç”Ÿæˆ

å®ŒæˆåŸºå‡†è¯„æµ‹å®ç°åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨EvalScopeæä¾›çš„å·¥å…·ç”Ÿæˆæ ‡å‡†æ–‡æ¡£ã€‚è¿™å°†ç¡®ä¿æ‚¨çš„åŸºå‡†è¯„æµ‹æœ‰ä¸€è‡´çš„æ–‡æ¡£æ ¼å¼ï¼Œå¹¶èƒ½å¤Ÿè¢«å…¶ä»–ç”¨æˆ·è½»æ¾ç†è§£å’Œä½¿ç”¨ã€‚

è¦ç”Ÿæˆä¸­è‹±æ–‡æ–‡æ¡£ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå°†æ ¹æ®æ³¨å†Œä¿¡æ¯ç”Ÿæˆæ–‡æ¡£ï¼š

```bash
pip install -e '.[docs]'
make docs
```

## 6. æäº¤PR
å®Œæˆè¿™äº›æ–¹æ³•çš„å®ç°å’Œæ–‡æ¡£ç”Ÿæˆåï¼Œæ‚¨çš„åŸºå‡†è¯„æµ‹å°±å‡†å¤‡å°±ç»ªäº†ï¼å¯ä»¥æäº¤[PR](https://github.com/modelscope/evalscope/pulls)äº†ã€‚åœ¨æäº¤ä¹‹å‰è¯·è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå°†è‡ªåŠ¨æ ¼å¼åŒ–ä»£ç ï¼š
```bash
make lint
```
ç¡®ä¿æ²¡æœ‰æ ¼å¼é—®é¢˜åï¼Œæˆ‘ä»¬å°†å°½å¿«åˆå¹¶ä½ çš„è´¡çŒ®ï¼Œè®©æ›´å¤šç”¨æˆ·æ¥ä½¿ç”¨ä½ è´¡çŒ®çš„åŸºå‡†è¯„æµ‹ã€‚å¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•æäº¤PRï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„[æŒ‡å—](https://github.com/modelscope/evalscope/blob/main/CONTRIBUTING.md)ï¼Œå¿«æ¥è¯•ä¸€è¯•å§ğŸš€
