# æ·»åŠ åŸºå‡†è¯„æµ‹

EvalScopeä½œä¸º[ModelScope](https://modelscope.cn)çš„å®˜æ–¹è¯„æµ‹å·¥å…·ï¼Œå…¶åŸºå‡†è¯„æµ‹åŠŸèƒ½æ­£åœ¨æŒç»­ä¼˜åŒ–ä¸­ï¼æˆ‘ä»¬è¯šé‚€æ‚¨å‚è€ƒæœ¬æ•™ç¨‹ï¼Œè½»æ¾æ·»åŠ è‡ªå·±çš„è¯„æµ‹åŸºå‡†ï¼Œå¹¶ä¸å¹¿å¤§ç¤¾åŒºæˆå‘˜åˆ†äº«æ‚¨çš„è´¡çŒ®ã€‚ä¸€èµ·åŠ©åŠ›EvalScopeçš„æˆé•¿ï¼Œè®©æˆ‘ä»¬çš„å·¥å…·æ›´åŠ å‡ºè‰²ï¼

ä¸‹é¢ä»¥`MMLU-Pro`ä¸ºä¾‹ï¼Œä»‹ç»å¦‚ä½•æ·»åŠ åŸºå‡†è¯„æµ‹ï¼Œä¸»è¦åŒ…å«ä¸Šä¼ æ•°æ®é›†ã€æ³¨å†Œæ•°æ®é›†ã€ç¼–å†™è¯„æµ‹ä»»åŠ¡ä¸‰ä¸ªæ­¥éª¤ã€‚

## ä¸Šä¼ åŸºå‡†è¯„æµ‹æ•°æ®é›†

ä¸Šä¼ åŸºå‡†è¯„æµ‹æ•°æ®é›†åˆ°ModelScopeï¼Œè¿™å¯ä»¥è®©ç”¨æˆ·ä¸€é”®åŠ è½½æ•°æ®é›†ï¼Œè®©æ›´å¤šç”¨æˆ·å—ç›Šã€‚å½“ç„¶ï¼Œå¦‚æœæ•°æ®é›†å·²ç»å­˜åœ¨ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ã€‚

```{seealso}
ä¾‹å¦‚ï¼š[modelscope/MMLU-Pro](https://modelscope.cn/datasets/modelscope/MMLU-Pro/summary)ï¼Œå‚è€ƒ[æ•°æ®é›†ä¸Šä¼ æ•™ç¨‹](https://www.modelscope.cn/docs/datasets/create)ã€‚
```

è¯·ç¡®ä¿æ•°æ®å¯ä»¥è¢«modelscopeåŠ è½½ï¼Œæµ‹è¯•ä»£ç å¦‚ä¸‹ï¼š

```python
from modelscope import MsDataset

dataset = MsDataset.load("modelscope/MMLU-Pro")  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†
```

## æ³¨å†ŒåŸºå‡†è¯„æµ‹

åœ¨EvalScopeä¸­æ·»åŠ åŸºå‡†è¯„æµ‹ã€‚

### åˆ›å»ºæ–‡ä»¶ç»“æ„

é¦–å…ˆ[Fork EvalScope](https://github.com/modelscope/evalscope/fork) ä»“åº“ï¼Œå³åˆ›å»ºä¸€ä¸ªè‡ªå·±çš„EvalScopeä»“åº“å‰¯æœ¬ï¼Œå°†å…¶cloneåˆ°æœ¬åœ°ã€‚

ç„¶åï¼Œåœ¨`evalscope/benchmarks/`ç›®å½•ä¸‹æ·»åŠ åŸºå‡†è¯„æµ‹ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
evalscope/benchmarks/
â”œâ”€â”€ benchmark_name
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ benchmark_name_adapter.py
â”‚   â””â”€â”€ ...
```
å…·ä½“åˆ°`MMLU-Pro`ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
evalscope/benchmarks/
â”œâ”€â”€ mmlu_pro
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mmlu_pro_adapter.py
â”‚   â””â”€â”€ ...
```

### æ³¨å†Œ`Benchmark`

æˆ‘ä»¬éœ€è¦åœ¨`benchmark_name_adapter.py`ä¸­æ³¨å†Œ`Benchmark`ï¼Œä½¿å¾—EvalScopeèƒ½å¤ŸåŠ è½½æˆ‘ä»¬æ·»åŠ çš„åŸºå‡†æµ‹è¯•ã€‚ä»¥`MMLU-Pro`ä¸ºä¾‹ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

- å¯¼å…¥`Benchmark`å’Œ`DataAdapter`
- æ³¨å†Œ`Benchmark`ï¼ŒæŒ‡å®šï¼š
    - `name`ï¼šåŸºå‡†æµ‹è¯•åç§°
    - `dataset_id`ï¼šåŸºå‡†æµ‹è¯•æ•°æ®é›†IDï¼Œç”¨äºåŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®é›†
    - `model_adapter`ï¼šåŸºå‡†æµ‹è¯•æ¨¡å‹é€‚é…å™¨ã€‚æ”¹æ¨¡å‹é€‚é…å™¨ç”¨äºæœ¬åœ°åŠ è½½æ¨¡å‹æ¨ç†ï¼Œæ”¯æŒä¸‰ç§ï¼š
        - `ChatGenerationModelAdapter`ï¼šé€šç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹è¯„æµ‹ï¼Œé€šè¿‡è¾“å…¥promptï¼Œè¿”å›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        - `MultiChoiceModelAdapter`ï¼šå¤šé€‰é¢˜è¯„æµ‹ï¼Œé€šè¿‡logitsæ¥è®¡ç®—é€‰é¡¹çš„æ¦‚ç‡ï¼Œè¿”å›æœ€å¤§æ¦‚ç‡é€‰é¡¹
        - `ContinuationLogitsModelAdapter`ï¼šå¤šé€‰æ–‡æœ¬è¯„æµ‹ï¼Œé€šè¿‡loglikelihoodæ¥è®¡ç®—æ¯ä¸ªä¸Šä¸‹æ–‡-å»¶ç»­å¯¹çš„å¯¹æ•°ä¼¼ç„¶å€¼ï¼Œè¿”å›å¯¹æ•°ä¼¼ç„¶å€¼åˆ—è¡¨
    - `subset_list`ï¼šåŸºå‡†æµ‹è¯•æ•°æ®é›†çš„å­æ•°æ®é›†
    - `metric_list`ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼°æŒ‡æ ‡
    - `few_shot_num`ï¼šCoTè¯„æµ‹çš„æ ·æœ¬æ•°é‡
    - `train_split`ï¼šåŸºå‡†æµ‹è¯•è®­ç»ƒé›†ï¼Œç”¨äºé‡‡æ ·CoTæ ·ä¾‹
    - `eval_split`ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼°é›†
    - `prompt_template`ï¼šåŸºå‡†æµ‹è¯•æç¤ºæ¨¡æ¿
- åˆ›å»º`MMLUProAdapter`ç±»ï¼Œç»§æ‰¿è‡ª`DataAdapter`ã€‚

```{tip}
`subset_list`, `train_split`, `eval_split` å¯ä»¥ä»æ•°æ®é›†é¢„è§ˆä¸­è·å–ï¼Œä¾‹å¦‚[MMLU-Proé¢„è§ˆ](https://modelscope.cn/datasets/modelscope/MMLU-Pro/dataPeview)

![MMLU-Proé¢„è§ˆ](./images/mmlu_pro_preview.png)
```

ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from evalscope.benchmarks import Benchmark, DataAdapter
from evalscope.metrics import WeightedAverageAccuracy
from evalscope.models import MultiChoiceModelAdapter


@Benchmark.register(
    name='mmlu_pro',
    dataset_id='modelscope/mmlu-pro',
    model_adapter=MultiChoiceModelAdapter,
    subset_list=['default'],
    metric_list=[WeightedAverageAccuracy],
    few_shot_num=0,
    train_split='validation',
    eval_split='test',
    prompt_template='You are an knowledge expert, you are supposed to answer the multi-choice question to derive your final answer as `The answer is ...`.',
)
class MMLUProAdapter(DataAdapter):

    def __init__(self, **kwargs):

        super().__init__(**kwargs)
```


## ç¼–å†™è¯„æµ‹é€»è¾‘

å®Œæˆ`DataAdapter`çš„ç¼–å†™ï¼Œå³å¯åœ¨EvalScopeä¸­æ·»åŠ è¯„æµ‹ä»»åŠ¡ã€‚éœ€è¦å®ç°å¦‚ä¸‹æ–¹æ³•ï¼š

- `gen_prompt`ï¼šç”Ÿæˆæ¨¡å‹è¾“å…¥prompt
    - å¯¹äºç±» `ChatGenerationModelAdapter`ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š`{'data': [full_prompt], 'system_prompt': (str, optional)}` å…¶ä¸­ `full_prompt: str`ï¼Œæ¯ä¸ªæ•°æ®æ ·æœ¬æ„é€ çš„æç¤ºã€‚

    - å¯¹äºç±» `MultiChoiceModelAdapter`ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š`{'data': [full_prompt], 'multi_choices': self.choices}` å…¶ä¸­ `full_prompt: str`ï¼Œæ¯ä¸ªæ•°æ®æ ·æœ¬æ„é€ çš„æç¤ºã€‚

    - å¯¹äºç±» `ContinuationEvalModelAdapter`ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š`{'data': ctx_continuation_pair_list, 'multi_choices': self.choices}` å…¶ä¸­ `ctx_continuation_pair_list: list`ï¼Œä¸Šä¸‹æ–‡-å»¶ç»­å¯¹çš„åˆ—è¡¨ã€‚

```{note}
è‹¥`gen_prompt`æä¾›çš„é€»è¾‘ä¸ç¬¦åˆé¢„æœŸï¼Œå¯ä»¥é‡å†™`gen_prompts`æ–¹æ³•ï¼Œæ¥è‡ªå®šä¹‰ä»æ•°æ®é›†åˆ°promptçš„è½¬æ¢é€»è¾‘ã€‚
```

- `get_gold_answer`ï¼šè§£ææ•°æ®é›†çš„æ ‡å‡†ç­”æ¡ˆ
- `parse_pred_result`ï¼šè§£ææ¨¡å‹è¾“å‡ºï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„eval_typeè¿”å›ä¸åŒçš„ç­”æ¡ˆè§£ææ–¹å¼
- `match`ï¼šåŒ¹é…æ¨¡å‹è¾“å‡ºå’Œæ•°æ®é›†æ ‡å‡†ç­”æ¡ˆï¼Œç»™å‡ºæ‰“åˆ†

å®Œæ•´ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
class MMLUProAdapter(DataAdapter):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        self.choices = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
        self.categories = ['computer science', 'math', 'chemistry', 'engineering', 'law', 'biology',
                            'health', 'physics', 'business', 'philosophy', 'economics', 'other',
                            'psychology', 'history']
        
    
    def gen_prompts(self, data_dict: dict, **kwargs) -> Dict[str, list]:
        """
        Generate model prompt from raw input, unify the prompt format for MMLU-Pro benchmark.
        Return a dict with category as key and list of prompts as value.
        """
        
        data_dict = data_dict[self.subset_list[0]]  # Only one subset for MMLU-Pro
        fewshot_prompts = self.get_fewshot_examples(data_dict)
        
        #  Use the category as key to group the prompts
        res_dict = defaultdict(list)
        # generate prompts for each test sample
        for entry in data_dict[self.eval_split]:
            prefix = fewshot_prompts[entry['category']]
            query = prefix + 'Q: ' + entry['question'] + '\n' + \
                self.__form_options(entry['options']) + '\n'
            
            prompt_d = {
                'data': [query],
                'system_prompt': self.prompt_template,
                AnswerKeys.RAW_INPUT: entry
            }
            
            res_dict[entry['category']].append(prompt_d)
        return res_dict
    
    def get_fewshot_examples(self, data_dict: dict):
        # load 5-shot prompts for each category
        prompts = {c: '' for c in self.categories}
        for d in data_dict[self.train_split]:
            prompts[d['category']] += 'Q:' + ' ' + d['question'] + '\n' + \
                self.__form_options(d['options']) + '\n' + \
                d['cot_content'] + '\n\n'
        return prompts
    
    
    def __form_options(self, options: list):
        option_str = 'Options are:\n'
        for opt, choice in zip(options, self.choices):
            option_str += f'({choice}): {opt}' + '\n'
        return option_str
    
    def get_gold_answer(self, input_d: dict) -> str:
        """
        Parse the raw input labels (gold).

        Args:
            input_d: input raw data. Depending on the dataset.

        Returns:
            The parsed input. e.g. gold answer ... Depending on the dataset.
        """
        return input_d['answer']


    def parse_pred_result(self, result: str, raw_input_d: dict = None, eval_type: str = EvalType.CHECKPOINT) -> str:
        """
        Parse the predicted result and extract proper answer.

        Args:
            result: Predicted answer from the model. Usually a string for chat.
            raw_input_d: The raw input. Depending on the dataset.
            eval_type: 'checkpoint' or 'service' or `custom`, default: 'checkpoint'

        Returns:
            The parsed answer. Depending on the dataset. Usually a string for chat.
        """
        return ResponseParser.parse_first_option(result)


    def match(self, gold: str, pred: str) -> float:
        """
        Match the gold answer and the predicted answer.

        Args:
            gold (Any): The golden answer. Usually a string for chat/multiple-choice-questions.
                        e.g. 'A', extracted from get_gold_answer method.
            pred (Any): The predicted answer. Usually a string for chat/multiple-choice-questions.
                        e.g. 'B', extracted from parse_pred_result method.

        Returns:
            The match result. Usually a score (float) for chat/multiple-choice-questions.
        """
        return exact_match(gold=gold, pred=pred)

```

## è¿è¡Œè¯„æµ‹

è°ƒè¯•ä»£ç ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œã€‚

```python
from evalscope import run_task
task_cfg = {'model': 'qwen/Qwen2-0.5B-Instruct',
            'datasets': ['mmlu_pro'],
            'limit': 2,
            'debug': True}
run_task(task_cfg=task_cfg)
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```text
+---------------------+-------------------------------------------+
| Model               | mmlu-pro                                  |
+=====================+===========================================+
| Qwen2-0.5B-Instruct | (mmlu-pro/WeightedAverageAccuracy) 0.1429 |
+---------------------+-------------------------------------------+ 
```

è¿è¡Œæ²¡é—®é¢˜çš„è¯ï¼Œå°±å¯ä»¥æäº¤[PR](https://github.com/modelscope/evalscope/pulls)äº†ï¼Œæˆ‘ä»¬å°†å°½å¿«åˆå¹¶ä½ çš„è´¡çŒ®ï¼Œè®©æ›´å¤šç”¨æˆ·æ¥ä½¿ç”¨ä½ è´¡çŒ®çš„åŸºå‡†è¯„æµ‹ï¼Œå¿«æ¥è¯•ä¸€è¯•å§ğŸš€
