# LLM Benchmarks

Below is the list of supported LLM benchmarks. Click on a benchmark name for details.

| Benchmark Name | Pretty Name | Task Categories |
|------------|----------|----------|
| `aa_lcr` | [AA-LCR](../../benchmarks/aa_lcr.md) | `Knowledge`, `LongContext`, `Reasoning` |
| `aime24` | [AIME-2024](../../benchmarks/aime24.md) | `Math`, `Reasoning` |
| `aime25` | [AIME-2025](../../benchmarks/aime25.md) | `Math`, `Reasoning` |
| `alpaca_eval` | [AlpacaEval2.0](../../benchmarks/alpaca_eval.md) | `Arena`, `InstructionFollowing` |
| `amc` | [AMC](../../benchmarks/amc.md) | `Math`, `Reasoning` |
| `anat_em` | [AnatEM](../../benchmarks/anat_em.md) | `Knowledge`, `NER` |
| `arc` | [ARC](../../benchmarks/arc.md) | `MCQ`, `Reasoning` |
| `arena_hard` | [ArenaHard](../../benchmarks/arena_hard.md) | `Arena`, `InstructionFollowing` |
| `bbh` | [BBH](../../benchmarks/bbh.md) | `Reasoning` |
| `bc2gm` | [BC2GM](../../benchmarks/bc2gm.md) | `Knowledge`, `NER` |
| `bc4chemd` | [BC4CHEMD](../../benchmarks/bc4chemd.md) | `Knowledge`, `NER` |
| `bc5cdr` | [BC5CDR](../../benchmarks/bc5cdr.md) | `Knowledge`, `NER` |
| `biomix_qa` | [BioMixQA](../../benchmarks/biomix_qa.md) | `Knowledge`, `MCQ`, `Medical` |
| `broad_twitter_corpus` | [BroadTwitterCorpus](../../benchmarks/broad_twitter_corpus.md) | `Knowledge`, `NER` |
| `ceval` | [C-Eval](../../benchmarks/ceval.md) | `Chinese`, `Knowledge`, `MCQ` |
| `chinese_simpleqa` | [Chinese-SimpleQA](../../benchmarks/chinese_simpleqa.md) | `Chinese`, `Knowledge`, `QA` |
| `cmmlu` | [C-MMLU](../../benchmarks/cmmlu.md) | `Chinese`, `Knowledge`, `MCQ` |
| `coin_flip` | [CoinFlip](../../benchmarks/coin_flip.md) | `Reasoning`, `Yes/No` |
| `commonsense_qa` | [CommonsenseQA](../../benchmarks/commonsense_qa.md) | `Commonsense`, `MCQ`, `Reasoning` |
| `competition_math` | [Competition-MATH](../../benchmarks/competition_math.md) | `Math`, `Reasoning` |
| `conll2003` | [CoNLL2003](../../benchmarks/conll2003.md) | `Knowledge`, `NER` |
| `conllpp` | [CoNLL++](../../benchmarks/conllpp.md) | `Knowledge`, `NER` |
| `copious` | [Copious](../../benchmarks/copious.md) | `Knowledge`, `NER` |
| `cross_ner` | [CrossNER](../../benchmarks/cross_ner.md) | `Knowledge`, `NER` |
| `data_collection` | [Data-Collection](../../benchmarks/data_collection.md) | `Custom` |
| `docmath` | [DocMath](../../benchmarks/docmath.md) | `LongContext`, `Math`, `Reasoning` |
| `drivel_binary` | [DrivelologyBinaryClassification](../../benchmarks/drivel_binary.md) | `Yes/No` |
| `drivel_multilabel` | [DrivelologyMultilabelClassification](../../benchmarks/drivel_multilabel.md) | `MCQ` |
| `drivel_selection` | [DrivelologyNarrativeSelection](../../benchmarks/drivel_selection.md) | `MCQ` |
| `drivel_writing` | [DrivelologyNarrativeWriting](../../benchmarks/drivel_writing.md) | `Knowledge`, `Reasoning` |
| `drop` | [DROP](../../benchmarks/drop.md) | `Reasoning` |
| `eq_bench` | [EQ-Bench](../../benchmarks/eq_bench.md) | `InstructionFollowing` |
| `fin_ner` | [FinNER](../../benchmarks/fin_ner.md) | `Knowledge`, `NER` |
| `frames` | [FRAMES](../../benchmarks/frames.md) | `LongContext`, `Reasoning` |
| `general_arena` | [GeneralArena](../../benchmarks/general_arena.md) | `Arena`, `Custom` |
| `general_mcq` | [General-MCQ](../../benchmarks/general_mcq.md) | `Custom`, `MCQ` |
| `general_qa` | [General-QA](../../benchmarks/general_qa.md) | `Custom`, `QA` |
| `genia_ner` | [GeniaNER](../../benchmarks/genia_ner.md) | `Knowledge`, `NER` |
| `gpqa_diamond` | [GPQA-Diamond](../../benchmarks/gpqa_diamond.md) | `Knowledge`, `MCQ` |
| `gsm8k` | [GSM8K](../../benchmarks/gsm8k.md) | `Math`, `Reasoning` |
| `halueval` | [HaluEval](../../benchmarks/halueval.md) | `Hallucination`, `Knowledge`, `Yes/No` |
| `harvey_ner` | [HarveyNER](../../benchmarks/harvey_ner.md) | `Knowledge`, `NER` |
| `health_bench` | [HealthBench](../../benchmarks/health_bench.md) | `Knowledge`, `Medical`, `QA` |
| `hellaswag` | [HellaSwag](../../benchmarks/hellaswag.md) | `Commonsense`, `Knowledge`, `MCQ` |
| `hle` | [Humanity's-Last-Exam](../../benchmarks/hle.md) | `Knowledge`, `QA` |
| `hmmt25` | [HMMT25](../../benchmarks/hmmt25.md) | `Math`, `Reasoning` |
| `humaneval` | [HumanEval](../../benchmarks/humaneval.md) | `Coding` |
| `humaneval_plus` | [HumanEvalPlus](../../benchmarks/humaneval_plus.md) | `Coding` |
| `ifbench` | [IFBench](../../benchmarks/ifbench.md) | `InstructionFollowing` |
| `ifeval` | [IFEval](../../benchmarks/ifeval.md) | `InstructionFollowing` |
| `iquiz` | [IQuiz](../../benchmarks/iquiz.md) | `Chinese`, `Knowledge`, `MCQ` |
| `jnlpba` | [JNLPBA](../../benchmarks/jnlpba.md) | `Knowledge`, `NER` |
| `jnlpba_rare` | [JNLPBA-Rare](../../benchmarks/jnlpba_rare.md) | `Knowledge`, `NER` |
| `live_code_bench` | [Live-Code-Bench](../../benchmarks/live_code_bench.md) | `Coding` |
| `logi_qa` | [LogiQA](../../benchmarks/logi_qa.md) | `MCQ`, `Reasoning` |
| `maritime_bench` | [MaritimeBench](../../benchmarks/maritime_bench.md) | `Chinese`, `Knowledge`, `MCQ` |
| `math_500` | [MATH-500](../../benchmarks/math_500.md) | `Math`, `Reasoning` |
| `math_qa` | [MathQA](../../benchmarks/math_qa.md) | `MCQ`, `Math`, `Reasoning` |
| `mbpp` | [MBPP](../../benchmarks/mbpp.md) | `Coding` |
| `mbpp_plus` | [MBPP-Plus](../../benchmarks/mbpp_plus.md) | `Coding` |
| `med_mcqa` | [Med-MCQA](../../benchmarks/med_mcqa.md) | `Knowledge`, `MCQ` |
| `mgsm` | [MGSM](../../benchmarks/mgsm.md) | `Math`, `MultiLingual`, `Reasoning` |
| `minerva_math` | [Minerva-Math](../../benchmarks/minerva_math.md) | `Math`, `Reasoning` |
| `mit_movie_trivia` | [MIT-Movie-Trivia](../../benchmarks/mit_movie_trivia.md) | `Knowledge`, `NER` |
| `mit_restaurant` | [MIT-Restaurant](../../benchmarks/mit_restaurant.md) | `Knowledge`, `NER` |
| `mmlu` | [MMLU](../../benchmarks/mmlu.md) | `Knowledge`, `MCQ` |
| `mmlu_pro` | [MMLU-Pro](../../benchmarks/mmlu_pro.md) | `Knowledge`, `MCQ` |
| `mmlu_redux` | [MMLU-Redux](../../benchmarks/mmlu_redux.md) | `Knowledge`, `MCQ` |
| `mri_mcqa` | [MRI-MCQA](../../benchmarks/mri_mcqa.md) | `Knowledge`, `MCQ`, `Medical` |
| `multi_if` | [Multi-IF](../../benchmarks/multi_if.md) | `InstructionFollowing`, `MultiLingual`, `MultiTurn` |
| `multi_nerd` | [MultiNERD](../../benchmarks/multi_nerd.md) | `Knowledge`, `NER` |
| `multiple_humaneval` | [MultiPL-E HumanEval](../../benchmarks/multiple_humaneval.md) | `Coding` |
| `multiple_mbpp` | [MultiPL-E MBPP](../../benchmarks/multiple_mbpp.md) | `Coding` |
| `music_trivia` | [MusicTrivia](../../benchmarks/music_trivia.md) | `Knowledge`, `MCQ` |
| `musr` | [MuSR](../../benchmarks/musr.md) | `MCQ`, `Reasoning` |
| `ncbi` | [NCBI](../../benchmarks/ncbi.md) | `Knowledge`, `NER` |
| `needle_haystack` | [Needle-in-a-Haystack](../../benchmarks/needle_haystack.md) | `LongContext`, `Retrieval` |
| `ontonotes5` | [OntoNotes5](../../benchmarks/ontonotes5.md) | `Knowledge`, `NER` |
| `openai_mrcr` | [OpenAI MRCR](../../benchmarks/openai_mrcr.md) | `LongContext`, `Retrieval` |
| `piqa` | [PIQA](../../benchmarks/piqa.md) | `Commonsense`, `MCQ`, `Reasoning` |
| `poly_math` | [PolyMath](../../benchmarks/poly_math.md) | `Math`, `MultiLingual`, `Reasoning` |
| `process_bench` | [ProcessBench](../../benchmarks/process_bench.md) | `Math`, `Reasoning` |
| `pubmedqa` | [PubMedQA](../../benchmarks/pubmedqa.md) | `Knowledge`, `Yes/No` |
| `qasc` | [QASC](../../benchmarks/qasc.md) | `Knowledge`, `MCQ` |
| `race` | [RACE](../../benchmarks/race.md) | `MCQ`, `Reasoning` |
| `refcoco` | [RefCOCO](../../benchmarks/refcoco.md) | `Grounding`, `ImageCaptioning`, `Knowledge`, `MultiModal` |
| `scicode` | [SciCode](../../benchmarks/scicode.md) | `Coding` |
| `sciq` | [SciQ](../../benchmarks/sciq.md) | `Knowledge`, `MCQ`, `ReadingComprehension` |
| `simple_qa` | [SimpleQA](../../benchmarks/simple_qa.md) | `Knowledge`, `QA` |
| `siqa` | [SIQA](../../benchmarks/siqa.md) | `Commonsense`, `MCQ`, `Reasoning` |
| `super_gpqa` | [SuperGPQA](../../benchmarks/super_gpqa.md) | `Knowledge`, `MCQ` |
| `swe_bench_lite` | [SWE-bench_Lite](../../benchmarks/swe_bench_lite.md) | `Coding` |
| `swe_bench_verified` | [SWE-bench_Verified](../../benchmarks/swe_bench_verified.md) | `Coding` |
| `swe_bench_verified_mini` | [SWE-bench_Verified_mini](../../benchmarks/swe_bench_verified_mini.md) | `Coding` |
| `terminal_bench_v2` | [Terminal-Bench-2.0](../../benchmarks/terminal_bench_v2.md) | `Coding` |
| `trivia_qa` | [TriviaQA](../../benchmarks/trivia_qa.md) | `QA`, `ReadingComprehension` |
| `truthful_qa` | [TruthfulQA](../../benchmarks/truthful_qa.md) | `Knowledge` |
| `tweebank_ner` | [TweeBankNER](../../benchmarks/tweebank_ner.md) | `Knowledge`, `NER` |
| `tweet_ner_7` | [TweetNER7](../../benchmarks/tweet_ner_7.md) | `Knowledge`, `NER` |
| `winogrande` | [Winogrande](../../benchmarks/winogrande.md) | `MCQ`, `Reasoning` |
| `wmt24pp` | [WMT2024++](../../benchmarks/wmt24pp.md) | `MachineTranslation`, `MultiLingual` |
| `wnut2017` | [WNUT2017](../../benchmarks/wnut2017.md) | `Knowledge`, `NER` |
| `zebralogicbench` | [ZebraLogicBench](../../benchmarks/zebralogicbench.md) | `Reasoning` |

:::{toctree}
:hidden:
:maxdepth: 1

../../benchmarks/aa_lcr.md
../../benchmarks/aime24.md
../../benchmarks/aime25.md
../../benchmarks/alpaca_eval.md
../../benchmarks/amc.md
../../benchmarks/anat_em.md
../../benchmarks/arc.md
../../benchmarks/arena_hard.md
../../benchmarks/bbh.md
../../benchmarks/bc2gm.md
../../benchmarks/bc4chemd.md
../../benchmarks/bc5cdr.md
../../benchmarks/biomix_qa.md
../../benchmarks/broad_twitter_corpus.md
../../benchmarks/ceval.md
../../benchmarks/chinese_simpleqa.md
../../benchmarks/cmmlu.md
../../benchmarks/coin_flip.md
../../benchmarks/commonsense_qa.md
../../benchmarks/competition_math.md
../../benchmarks/conll2003.md
../../benchmarks/conllpp.md
../../benchmarks/copious.md
../../benchmarks/cross_ner.md
../../benchmarks/data_collection.md
../../benchmarks/docmath.md
../../benchmarks/drivel_binary.md
../../benchmarks/drivel_multilabel.md
../../benchmarks/drivel_selection.md
../../benchmarks/drivel_writing.md
../../benchmarks/drop.md
../../benchmarks/eq_bench.md
../../benchmarks/fin_ner.md
../../benchmarks/frames.md
../../benchmarks/general_arena.md
../../benchmarks/general_mcq.md
../../benchmarks/general_qa.md
../../benchmarks/genia_ner.md
../../benchmarks/gpqa_diamond.md
../../benchmarks/gsm8k.md
../../benchmarks/halueval.md
../../benchmarks/harvey_ner.md
../../benchmarks/health_bench.md
../../benchmarks/hellaswag.md
../../benchmarks/hle.md
../../benchmarks/hmmt25.md
../../benchmarks/humaneval.md
../../benchmarks/humaneval_plus.md
../../benchmarks/ifbench.md
../../benchmarks/ifeval.md
../../benchmarks/iquiz.md
../../benchmarks/jnlpba.md
../../benchmarks/jnlpba_rare.md
../../benchmarks/live_code_bench.md
../../benchmarks/logi_qa.md
../../benchmarks/maritime_bench.md
../../benchmarks/math_500.md
../../benchmarks/math_qa.md
../../benchmarks/mbpp.md
../../benchmarks/mbpp_plus.md
../../benchmarks/med_mcqa.md
../../benchmarks/mgsm.md
../../benchmarks/minerva_math.md
../../benchmarks/mit_movie_trivia.md
../../benchmarks/mit_restaurant.md
../../benchmarks/mmlu.md
../../benchmarks/mmlu_pro.md
../../benchmarks/mmlu_redux.md
../../benchmarks/mri_mcqa.md
../../benchmarks/multi_if.md
../../benchmarks/multi_nerd.md
../../benchmarks/multiple_humaneval.md
../../benchmarks/multiple_mbpp.md
../../benchmarks/music_trivia.md
../../benchmarks/musr.md
../../benchmarks/ncbi.md
../../benchmarks/needle_haystack.md
../../benchmarks/ontonotes5.md
../../benchmarks/openai_mrcr.md
../../benchmarks/piqa.md
../../benchmarks/poly_math.md
../../benchmarks/process_bench.md
../../benchmarks/pubmedqa.md
../../benchmarks/qasc.md
../../benchmarks/race.md
../../benchmarks/refcoco.md
../../benchmarks/scicode.md
../../benchmarks/sciq.md
../../benchmarks/simple_qa.md
../../benchmarks/siqa.md
../../benchmarks/super_gpqa.md
../../benchmarks/swe_bench_lite.md
../../benchmarks/swe_bench_verified.md
../../benchmarks/swe_bench_verified_mini.md
../../benchmarks/terminal_bench_v2.md
../../benchmarks/trivia_qa.md
../../benchmarks/truthful_qa.md
../../benchmarks/tweebank_ner.md
../../benchmarks/tweet_ner_7.md
../../benchmarks/winogrande.md
../../benchmarks/wmt24pp.md
../../benchmarks/wnut2017.md
../../benchmarks/zebralogicbench.md
:::