# VLM Benchmarks

Below is the list of supported VLM benchmarks. Click on a benchmark name for details.

| Benchmark Name | Pretty Name | Task Categories |
|------------|----------|----------|
| `a_okvqa` | [A-OKVQA](../../benchmarks/a_okvqa.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `ai2d` | [AI2D](../../benchmarks/ai2d.md) | `Knowledge`, `MultiModal`, `QA` |
| `blink` | [BLINK](../../benchmarks/blink.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `cc_bench` | [CCBench](../../benchmarks/cc_bench.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `chartqa` | [ChartQA](../../benchmarks/chartqa.md) | `Knowledge`, `MultiModal`, `QA` |
| `cmmmu` | [CMMMU](../../benchmarks/cmmmu.md) | `Chinese`, `Knowledge`, `MultiModal`, `QA` |
| `cmmu` | [CMMU](../../benchmarks/cmmu.md) | `Knowledge`, `MCQ`, `MultiModal`, `QA` |
| `docvqa` | [DocVQA](../../benchmarks/docvqa.md) | `Knowledge`, `MultiModal`, `QA` |
| `fleurs` | [FLEURS](../../benchmarks/fleurs.md) | `Audio`, `MultiLingual`, `SpeechRecognition` |
| `general_vmcq` | [General-VMCQ](../../benchmarks/general_vmcq.md) | `Custom`, `MCQ`, `MultiModal` |
| `general_vqa` | [General-VQA](../../benchmarks/general_vqa.md) | `Custom`, `MultiModal`, `QA` |
| `gsm8k_v` | [GSM8K-V](../../benchmarks/gsm8k_v.md) | `Math`, `MultiModal`, `Reasoning` |
| `hallusion_bench` | [HallusionBench](../../benchmarks/hallusion_bench.md) | `Hallucination`, `MultiModal`, `Yes/No` |
| `infovqa` | [InfoVQA](../../benchmarks/infovqa.md) | `Knowledge`, `MultiModal`, `QA` |
| `librispeech` | [LibriSpeech](../../benchmarks/librispeech.md) | `Audio`, `SpeechRecognition` |
| `math_verse` | [MathVerse](../../benchmarks/math_verse.md) | `MCQ`, `Math`, `MultiModal`, `Reasoning` |
| `math_vision` | [MathVision](../../benchmarks/math_vision.md) | `MCQ`, `Math`, `MultiModal`, `Reasoning` |
| `math_vista` | [MathVista](../../benchmarks/math_vista.md) | `MCQ`, `Math`, `MultiModal`, `Reasoning` |
| `micro_vqa` | [MicroVQA](../../benchmarks/micro_vqa.md) | `Knowledge`, `MCQ`, `Medical`, `MultiModal` |
| `mm_bench` | [MMBench](../../benchmarks/mm_bench.md) | `Knowledge`, `MultiModal`, `QA` |
| `mm_star` | [MMStar](../../benchmarks/mm_star.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `mmmu` | [MMMU](../../benchmarks/mmmu.md) | `Knowledge`, `MultiModal`, `QA` |
| `mmmu_pro` | [MMMU-PRO](../../benchmarks/mmmu_pro.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `ocr_bench` | [OCRBench](../../benchmarks/ocr_bench.md) | `Knowledge`, `MultiModal`, `QA` |
| `ocr_bench_v2` | [OCRBench-v2](../../benchmarks/ocr_bench_v2.md) | `Knowledge`, `MultiModal`, `QA` |
| `olympiad_bench` | [OlympiadBench](../../benchmarks/olympiad_bench.md) | `Math`, `Reasoning` |
| `omni_bench` | [OmniBench](../../benchmarks/omni_bench.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `omni_doc_bench` | [OmniDocBench](../../benchmarks/omni_doc_bench.md) | `Knowledge`, `MultiModal`, `QA` |
| `pope` | [POPE](../../benchmarks/pope.md) | `Hallucination`, `MultiModal`, `Yes/No` |
| `real_world_qa` | [RealWorldQA](../../benchmarks/real_world_qa.md) | `Knowledge`, `MultiModal`, `QA` |
| `science_qa` | [ScienceQA](../../benchmarks/science_qa.md) | `Knowledge`, `MCQ`, `MultiModal` |
| `seed_bench_2_plus` | [SEED-Bench-2-Plus](../../benchmarks/seed_bench_2_plus.md) | `Knowledge`, `MCQ`, `MultiModal`, `Reasoning` |
| `simple_vqa` | [SimpleVQA](../../benchmarks/simple_vqa.md) | `MultiModal`, `QA`, `Reasoning` |
| `torgo` | [TORGO](../../benchmarks/torgo.md) | `Audio`, `SpeechRecognition` |
| `visulogic` | [VisuLogic](../../benchmarks/visulogic.md) | `MCQ`, `Math`, `MultiModal`, `Reasoning` |
| `vstar_bench` | [V*Bench](../../benchmarks/vstar_bench.md) | `Grounding`, `MCQ`, `MultiModal` |
| `zerobench` | [ZeroBench](../../benchmarks/zerobench.md) | `Knowledge`, `MultiModal`, `QA` |

:::{toctree}
:hidden:
:maxdepth: 1

../../benchmarks/a_okvqa.md
../../benchmarks/ai2d.md
../../benchmarks/blink.md
../../benchmarks/cc_bench.md
../../benchmarks/chartqa.md
../../benchmarks/cmmmu.md
../../benchmarks/cmmu.md
../../benchmarks/docvqa.md
../../benchmarks/fleurs.md
../../benchmarks/general_vmcq.md
../../benchmarks/general_vqa.md
../../benchmarks/gsm8k_v.md
../../benchmarks/hallusion_bench.md
../../benchmarks/infovqa.md
../../benchmarks/librispeech.md
../../benchmarks/math_verse.md
../../benchmarks/math_vision.md
../../benchmarks/math_vista.md
../../benchmarks/micro_vqa.md
../../benchmarks/mm_bench.md
../../benchmarks/mm_star.md
../../benchmarks/mmmu.md
../../benchmarks/mmmu_pro.md
../../benchmarks/ocr_bench.md
../../benchmarks/ocr_bench_v2.md
../../benchmarks/olympiad_bench.md
../../benchmarks/omni_bench.md
../../benchmarks/omni_doc_bench.md
../../benchmarks/pope.md
../../benchmarks/real_world_qa.md
../../benchmarks/science_qa.md
../../benchmarks/seed_bench_2_plus.md
../../benchmarks/simple_vqa.md
../../benchmarks/torgo.md
../../benchmarks/visulogic.md
../../benchmarks/vstar_bench.md
../../benchmarks/zerobench.md
:::