{
  "humaneval": {
    "meta": {
      "pretty_name": "HumanEval",
      "dataset_id": "opencompass/humaneval",
      "paper_url": null,
      "tags": [
        "Coding"
      ],
      "metrics": [
        "acc"
      ],
      "few_shot_num": 0,
      "eval_split": "test",
      "train_split": "",
      "subset_list": [
        "openai_humaneval"
      ],
      "description": "HumanEval is a benchmark for evaluating the ability of code generation models to write Python functions based on given specifications. It consists of programming tasks with a defined input-output behavior. **By default the code is executed in local environment. We recommend using sandbox execution to safely run and evaluate the generated code, please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html) for more details.**",
      "prompt_template": "Read the following function signature and docstring, and fully implement the function described. Your response should only contain the code for this function.\n{question}",
      "system_prompt": "",
      "few_shot_prompt_template": "",
      "aggregation": "mean_and_pass_at_k",
      "extra_params": {},
      "sandbox_config": {
        "image": "python:3.11-slim",
        "tools_config": {
          "shell_executor": {},
          "python_executor": {}
        }
      }
    },
    "statistics": {},
    "sample_example": {},
    "readme": {
      "en": "# HumanEval\n\nHumanEval is a benchmark for evaluating the ability of code generation models to write Python functions based on given specifications. It consists of programming tasks with a defined input-output behavior. **By default the code is executed in local environment. We recommend using sandbox execution to safely run and evaluate the generated code, please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html) for more details.**\n\n## Overview\n\n| Property | Value |\n|----------|-------|\n| **Benchmark Name** | `humaneval` |\n| **Dataset ID** | [opencompass/humaneval](https://modelscope.cn/datasets/opencompass/humaneval/summary) |\n| **Paper** | N/A |\n| **Tags** | `Coding` |\n| **Metrics** | `acc` |\n| **Default Shots** | 0-shot |\n| **Evaluation Split** | `test` |\n| **Aggregation** | `mean_and_pass_at_k` |\n\n\n## Data Statistics\n\n*Statistics not available.*\n\n## Sample Example\n\n*Sample example not available.*\n\n## Prompt Template\n\n**Prompt Template:**\n```text\nRead the following function signature and docstring, and fully implement the function described. Your response should only contain the code for this function.\n{question}\n```\n\n## Sandbox Configuration\n\nThis benchmark requires a sandbox environment for code execution.\n\n```json\n{\n  \"image\": \"python:3.11-slim\",\n  \"tools_config\": {\n    \"shell_executor\": {},\n    \"python_executor\": {}\n  }\n}\n```\n\n## Usage\n\n```python\nfrom evalscope import run_task\n\nresults = run_task(\n    model='your-model',\n    datasets=['humaneval'],\n)\n```\n\n---\n*Auto-generated by EvalScope on 2026-01-27 19:02:25*\n",
      "zh": "",
      "content_hash": "a13f9d867687a326330383dabcc62f96",
      "needs_translation": true
    },
    "updated_at": "2026-01-27T19:02:25.556945"
  }
}