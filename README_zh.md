[English](README.md) | ç®€ä½“ä¸­æ–‡

<p align="center">
<a href="https://pypi.org/project/llmuses"><img alt="PyPI - Downloads" src="https://img.shields.io/pypi/dm/llmuses">
</a>
<a href="https://github.com/modelscope/eval-scope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<p>

## ğŸ“– ç›®å½•
- [ç®€ä»‹](#ç®€ä»‹)
- [æ–°é—»](#æ–°é—»)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ•°æ®é›†åˆ—è¡¨](#æ•°æ®é›†åˆ—è¡¨)
- [Leaderboardæ¦œå•](#leaderboard-æ¦œå•)
- [å®éªŒå’ŒæŠ¥å‘Š](#å®éªŒå’ŒæŠ¥å‘Š)
- [æ€§èƒ½è¯„æµ‹å·¥å…·](#æ€§èƒ½è¯„æµ‹å·¥å…·)


## ğŸ“ ç®€ä»‹
å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ï¼ˆLLMs evaluationï¼‰å·²æˆä¸ºè¯„ä»·å’Œæ”¹è¿›å¤§æ¨¡å‹çš„é‡è¦æµç¨‹å’Œæ‰‹æ®µï¼Œä¸ºäº†æ›´å¥½åœ°æ”¯æŒå¤§æ¨¡å‹çš„è¯„æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†Eval-Scopeæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- é¢„ç½®äº†å¤šä¸ªå¸¸ç”¨çš„æµ‹è¯•åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…æ‹¬ï¼šMMLUã€CMMLUã€C-Evalã€GSM8Kã€ARCã€HellaSwagã€TruthfulQAã€MATHã€HumanEvalç­‰
- å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡ï¼ˆmetricsï¼‰çš„å®ç°
- ç»Ÿä¸€modelæ¥å…¥ï¼Œå…¼å®¹å¤šä¸ªç³»åˆ—æ¨¡å‹çš„generateã€chatæ¥å£
- è‡ªåŠ¨è¯„ä¼°ï¼ˆevaluatorï¼‰ï¼š
    - å®¢è§‚é¢˜è‡ªåŠ¨è¯„ä¼°
    - ä½¿ç”¨ä¸“å®¶æ¨¡å‹å®ç°å¤æ‚ä»»åŠ¡çš„è‡ªåŠ¨è¯„ä¼°
- è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ
- ç«æŠ€åœºæ¨¡å¼(Arena)
- å¯è§†åŒ–å·¥å…·
- [æ¨¡å‹æ€§èƒ½è¯„ä¼°](llmuses/perf/README.md)
- æ”¯æŒOpenCompassä½œä¸ºè¯„æµ‹åæ®µï¼Œå¯¹å…¶è¿›è¡Œäº†é«˜çº§å°è£…å’Œä»»åŠ¡ç®€åŒ–ï¼Œæ‚¨å¯ä»¥æ›´è½»æ¾åœ°æäº¤ä»»åŠ¡åˆ°OpenCompassè¿›è¡Œè¯„ä¼°ã€‚
- æ”¯æŒVLMEvalKitä½œä¸ºè¯„æµ‹åç«¯ï¼Œé€šè¿‡Eval-Scopeä½œä¸ºå…¥å£ï¼Œå‘èµ·VLMEvalKitçš„å¤šæ¨¡æ€è¯„æµ‹ä»»åŠ¡ï¼Œæ”¯æŒå¤šç§å¤šæ¨¡æ€æ¨¡å‹å’Œæ•°æ®é›†ã€‚
- å…¨é“¾è·¯æ”¯æŒï¼šé€šè¿‡ä¸SWIFTçš„æ— ç¼é›†æˆï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°è®­ç»ƒå’Œéƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œå‘èµ·è¯„æµ‹ä»»åŠ¡ï¼ŒæŸ¥çœ‹è¯„æµ‹æŠ¥å‘Šï¼Œå®ç°ä¸€ç«™å¼å¤§æ¨¡å‹å¼€å‘æµç¨‹ã€‚

**ç‰¹ç‚¹**
- è½»é‡åŒ–ï¼Œå°½é‡å‡å°‘ä¸å¿…è¦çš„æŠ½è±¡å’Œé…ç½®
- æ˜“äºå®šåˆ¶
  - ä»…éœ€å®ç°ä¸€ä¸ªç±»å³å¯æ¥å…¥æ–°çš„æ•°æ®é›†
  - æ¨¡å‹å¯æ‰˜ç®¡åœ¨[ModelScope](https://modelscope.cn)ä¸Šï¼Œä»…éœ€model idå³å¯ä¸€é”®å‘èµ·è¯„æµ‹
  - æ”¯æŒæœ¬åœ°æ¨¡å‹å¯éƒ¨ç½²åœ¨æœ¬åœ°
  - è¯„ä¼°æŠ¥å‘Šå¯è§†åŒ–å±•ç°
- ä¸°å¯Œçš„è¯„ä¼°æŒ‡æ ‡
- model-basedè‡ªåŠ¨è¯„ä¼°æµç¨‹ï¼Œæ”¯æŒå¤šç§è¯„ä¼°æ¨¡å¼
  - Single mode: ä¸“å®¶æ¨¡å‹å¯¹å•ä¸ªæ¨¡å‹æ‰“åˆ†
  - Pairwise-baseline mode: ä¸ baseline æ¨¡å‹å¯¹æ¯”
  - Pairwise (all) mode: å…¨éƒ¨æ¨¡å‹ä¸¤ä¸¤å¯¹æ¯”


## ğŸ‰ æ–°é—»
- **[2024.07.26]:** æ”¯æŒ**VLMEvalKit**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œå‘èµ·å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä»»åŠ¡ï¼Œ[ä½¿ç”¨æŒ‡å—](#vlmevalkit-è¯„æµ‹åç«¯) ğŸ”¥ğŸ”¥ğŸ”¥
- **[2024.06.29]:** æ”¯æŒ**OpenCompass**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†é«˜çº§å°è£…ï¼Œæ”¯æŒpipæ–¹å¼å®‰è£…ï¼Œç®€åŒ–äº†è¯„ä¼°ä»»åŠ¡é…ç½®ï¼Œ[ä½¿ç”¨æŒ‡å—](#opencompass-è¯„æµ‹åç«¯) ğŸ”¥ğŸ”¥ğŸ”¥
- **[2024.06.13]:** Eval-Scopeä¸å¾®è°ƒæ¡†æ¶SWIFTè¿›è¡Œæ— ç¼å¯¹æ¥ï¼Œæä¾›LLMä»è®­ç»ƒåˆ°è¯„æµ‹çš„å…¨é“¾è·¯æ”¯æŒ ğŸš€ğŸš€ğŸš€
- **[2024.06.13]:** æ¥å…¥Agentè¯„æµ‹é›†ToolBench ğŸš€ğŸš€ğŸš€



## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡
### ä½¿ç”¨pipå®‰è£…
æˆ‘ä»¬æ¨èä½¿ç”¨condaæ¥ç®¡ç†ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨pipå®‰è£…ä¾èµ–:
1. åˆ›å»ºcondaç¯å¢ƒ
```shell
conda create -n eval-scope python=3.10
conda activate eval-scope
```
2. å®‰è£…ä¾èµ–
```shell
pip install llmuses
```

### ä½¿ç”¨æºç å®‰è£…
1. ä¸‹è½½æºç 
```shell
git clone https://github.com/modelscope/eval-scope.git
```
2. å®‰è£…ä¾èµ–
```shell
cd eval-scope/
pip install -e .
```


## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç®€å•è¯„ä¼°
åœ¨æŒ‡å®šçš„è‹¥å¹²æ•°æ®é›†ä¸Šè¯„ä¼°æŸä¸ªæ¨¡å‹ï¼Œæµç¨‹å¦‚ä¸‹ï¼š
å¦‚æœä½¿ç”¨gitå®‰è£…ï¼Œå¯åœ¨ä»»æ„è·¯å¾„ä¸‹æ‰§è¡Œï¼š
```shell
python -m llmuses.run --model ZhipuAI/chatglm3-6b --template-type chatglm3 --datasets arc --limit 100
```
å¦‚æœä½¿ç”¨æºç å®‰è£…ï¼Œåœ¨eval-scopeè·¯å¾„ä¸‹æ‰§è¡Œï¼š
```shell
python llmuses/run.py --model ZhipuAI/chatglm3-6b --template-type chatglm3 --datasets mmlu ceval --limit 10
```
å…¶ä¸­ï¼Œ--modelå‚æ•°æŒ‡å®šäº†æ¨¡å‹çš„ModelScope model idï¼Œæ¨¡å‹é“¾æ¥ï¼š[ZhipuAI/chatglm3-6b](https://modelscope.cn/models/ZhipuAI/chatglm3-6b/summary)

### å¸¦å‚æ•°è¯„ä¼°
```shell
python llmuses/run.py --model ZhipuAI/chatglm3-6b --template-type chatglm3 --model-args revision=v1.0.2,precision=torch.float16,device_map=auto --datasets mmlu ceval --use-cache true --limit 10
```
```
python llmuses/run.py --model qwen/Qwen-1_8B --generation-config do_sample=false,temperature=0.0 --datasets ceval --dataset-args '{"ceval": {"few_shot_num": 0, "few_shot_random": false}}' --limit 10
```
å‚æ•°è¯´æ˜ï¼š
- --model-args: æ¨¡å‹å‚æ•°ï¼Œä»¥é€—å·åˆ†éš”ï¼Œkey=valueå½¢å¼
- --datasets: æ•°æ®é›†åç§°ï¼Œæ”¯æŒè¾“å…¥å¤šä¸ªæ•°æ®é›†ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†å¼€ï¼Œå‚è€ƒä¸‹æ–‡`æ•°æ®é›†åˆ—è¡¨`ç« èŠ‚
- --use-cache: æ˜¯å¦ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé»˜è®¤ä¸º`false`;å¦‚æœä¸º`true`ï¼Œåˆ™å·²ç»è¯„ä¼°è¿‡çš„æ¨¡å‹å’Œæ•°æ®é›†ç»„åˆå°†ä¸ä¼šå†æ¬¡è¯„ä¼°ï¼Œç›´æ¥ä»æœ¬åœ°ç¼“å­˜è¯»å–
- --dataset-args: æ•°æ®é›†çš„evaluation settingsï¼Œä»¥jsonæ ¼å¼ä¼ å…¥ï¼Œkeyä¸ºæ•°æ®é›†åç§°ï¼Œvalueä¸ºå‚æ•°ï¼Œæ³¨æ„éœ€è¦è·Ÿ--datasetså‚æ•°ä¸­çš„å€¼ä¸€ä¸€å¯¹åº”
  - --few_shot_num: few-shotçš„æ•°é‡
  - --few_shot_random: æ˜¯å¦éšæœºé‡‡æ ·few-shotæ•°æ®ï¼Œå¦‚æœä¸è®¾ç½®ï¼Œåˆ™é»˜è®¤ä¸ºtrue
- --limit: æ¯ä¸ªsubsetæœ€å¤§è¯„ä¼°æ•°æ®é‡
- --template-type: éœ€è¦æ‰‹åŠ¨æŒ‡å®šè¯¥å‚æ•°ï¼Œä½¿å¾—eval-scopeèƒ½å¤Ÿæ­£ç¡®è¯†åˆ«æ¨¡å‹çš„ç±»å‹ï¼Œç”¨æ¥è®¾ç½®model generation configã€‚  

å…³äº--template-typeï¼Œå…·ä½“å¯å‚è€ƒï¼š[æ¨¡å‹ç±»å‹åˆ—è¡¨](https://github.com/modelscope/swift/blob/main/docs/source/LLM/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.md)
åœ¨æ¨¡å‹åˆ—è¡¨ä¸­çš„`Default Template`å­—æ®µä¸­æ‰¾åˆ°åˆé€‚çš„templateï¼›  
å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼Œæ¥æŸ¥çœ‹æ¨¡å‹çš„template type listï¼š
```shell
from llmuses.models.template import TemplateType
print(TemplateType.get_template_name_list())
```

### ä½¿ç”¨è¯„æµ‹åç«¯ (Evaluation Backend)
Eval-Scopeæ”¯æŒä½¿ç”¨ç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶å‘èµ·è¯„æµ‹ä»»åŠ¡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºè¯„æµ‹åç«¯ (Evaluation Backend)ã€‚ç›®å‰æ”¯æŒçš„Evaluation Backendæœ‰ï¼š
- **Native**ï¼šEval-Scopeè‡ªèº«çš„**é»˜è®¤è¯„æµ‹æ¡†æ¶**ï¼Œæ”¯æŒå¤šç§è¯„ä¼°æ¨¡å¼ï¼ŒåŒ…æ‹¬å•æ¨¡å‹è¯„ä¼°ã€ç«æŠ€åœºæ¨¡å¼ã€Baselineæ¨¡å‹å¯¹æ¯”æ¨¡å¼ç­‰ã€‚
- [OpenCompass](https://github.com/open-compass/opencompass)ï¼šé€šè¿‡Eval-Scopeä½œä¸ºå…¥å£ï¼Œå‘èµ·OpenCompassçš„è¯„æµ‹ä»»åŠ¡ï¼Œè½»é‡çº§ã€æ˜“äºå®šåˆ¶ã€æ”¯æŒä¸LLMå¾®è°ƒæ¡†æ¶[ModelScope Swift](https://github.com/modelscope/swift)çš„æ— ç¼é›†æˆã€‚
- [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)ï¼šé€šè¿‡Eval-Scopeä½œä¸ºå…¥å£ï¼Œå‘èµ·VLMEvalKitçš„å¤šæ¨¡æ€è¯„æµ‹ä»»åŠ¡ï¼Œæ”¯æŒå¤šç§å¤šæ¨¡æ€æ¨¡å‹å’Œæ•°æ®é›†ï¼Œæ”¯æŒä¸LLMå¾®è°ƒæ¡†æ¶[ModelScope Swift](https://github.com/modelscope/swift)çš„æ— ç¼é›†æˆã€‚
- **ThirdParty**: ç¬¬ä¸‰æ–¹è¯„ä¼°ä»»åŠ¡ï¼Œå¦‚[ToolBench](llmuses/thirdparty/toolbench/README.md)ã€‚

#### OpenCompass è¯„æµ‹åç«¯

ä¸ºä¾¿äºä½¿ç”¨OpenCompass è¯„æµ‹åç«¯ï¼Œæˆ‘ä»¬åŸºäºOpenCompassæºç åšäº†å®šåˆ¶ï¼Œå‘½åä¸º`ms-opencompass`ï¼Œè¯¥ç‰ˆæœ¬åœ¨åŸç‰ˆåŸºç¡€ä¸Šå¯¹è¯„ä¼°ä»»åŠ¡çš„é…ç½®å’Œæ‰§è¡Œåšäº†ä¸€äº›ä¼˜åŒ–ï¼Œå¹¶æ”¯æŒpypiå®‰è£…æ–¹å¼ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡Eval-Scopeå‘èµ·è½»é‡åŒ–çš„OpenCompassè¯„ä¼°ä»»åŠ¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å…ˆæœŸå¼€æ”¾äº†åŸºäºOpenAI APIæ ¼å¼çš„æ¥å£è¯„ä¼°ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[ModelScope Swift](https://github.com/modelscope/swift) éƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œå…¶ä¸­ï¼Œ[swift deploy](https://swift.readthedocs.io/zh-cn/latest/LLM/VLLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E4%B8%8E%E9%83%A8%E7%BD%B2.html#vllm)æ”¯æŒä½¿ç”¨vLLMæ‹‰èµ·æ¨¡å‹æ¨ç†æœåŠ¡ã€‚

##### å®‰è£…
```shell
# å®‰è£…é¢å¤–é€‰é¡¹
pip install llmuses[opencompass]
```

##### æ•°æ®å‡†å¤‡
ç›®å‰æ”¯æŒçš„æ•°æ®é›†æœ‰ï¼š
```python
'obqa', 'AX_b', 'siqa', 'nq', 'mbpp', 'winogrande', 'mmlu', 'BoolQ', 'cluewsc', 'ocnli', 'lambada', 'CMRC', 'ceval', 'csl', 'cmnli', 'bbh', 'ReCoRD', 'math', 'humaneval', 'eprstmt', 'WSC', 'storycloze', 'MultiRC', 'RTE', 'chid', 'gsm8k', 'AX_g', 'bustm', 'afqmc', 'piqa', 'lcsts', 'strategyqa', 'Xsum', 'agieval', 'ocnli_fc', 'C3', 'tnews', 'race', 'triviaqa', 'CB', 'WiC', 'hellaswag', 'summedits', 'GaokaoBench', 'ARC_e', 'COPA', 'ARC_c', 'DRCD'
```
æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚è€ƒ[OpenCompassæ•°æ®é›†åˆ—è¡¨](https://hub.opencompass.org.cn/home)
æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼Œæ¥æŸ¥çœ‹æ•°æ®é›†çš„åç§°åˆ—è¡¨ï¼š
```python
from llmuses.backend.opencompass import OpenCompassBackendManager
print(f'** All datasets from OpenCompass backend: {OpenCompassBackendManager.list_datasets()}')
```

æ•°æ®é›†ä¸‹è½½æ–¹å¼ï¼š
- æ–¹å¼1ï¼šä½¿ç”¨ModelScopeæ•°æ®é›†ä¸‹è½½
    ```shell
    git clone https://www.modelscope.cn/datasets/swift/evalscope_resource.git
    ```

- æ–¹å¼2ï¼šä½¿ç”¨githubé“¾æ¥ä¸‹è½½
    ```shell
    wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-complete-20240207.zip
    ```
æ€»å¤§å°çº¦1.7GBï¼Œä¸‹è½½å¹¶è§£å‹åï¼Œå°†æ•°æ®é›†æ–‡ä»¶å¤¹ï¼ˆå³dataæ–‡ä»¶å¤¹ï¼‰æ”¾ç½®åœ¨å½“å‰å·¥ä½œè·¯å¾„ä¸‹ã€‚åç»­æˆ‘ä»¬ä¹Ÿå³å°†æ”¯æŒæ‰˜ç®¡åœ¨ModelScopeä¸Šçš„æ•°æ®é›†æŒ‰éœ€åŠ è½½æ–¹å¼ã€‚


##### æ¨¡å‹æ¨ç†æœåŠ¡
æˆ‘ä»¬ä½¿ç”¨ModelScope swiftéƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œå…·ä½“å¯å‚è€ƒï¼š[ModelScope Swiftéƒ¨ç½²æŒ‡å—](https://swift.readthedocs.io/zh-cn/latest/LLM/VLLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E4%B8%8E%E9%83%A8%E7%BD%B2.html#vllm)
```shell
# å®‰è£…ms-swift
pip install ms-swift

# éƒ¨ç½²æ¨¡å‹æœåŠ¡
CUDA_VISIBLE_DEVICES=0 swift deploy --model_type llama3-8b-instruct --port 8000
```


##### æ¨¡å‹è¯„ä¼°

å‚è€ƒç¤ºä¾‹æ–‡ä»¶ï¼š [example_eval_swift_openai_api](examples/example_eval_swift_openai_api.py) æ¥é…ç½®è¯„ä¼°ä»»åŠ¡
æ‰§è¡Œè¯„ä¼°ä»»åŠ¡ï¼š
```shell
python examples/example_eval_swift_openai_api.py
```


#### VLMEvalKit è¯„æµ‹åç«¯

ä¸ºä¾¿äºä½¿ç”¨VLMEvalKit è¯„æµ‹åç«¯ï¼Œæˆ‘ä»¬åŸºäºVLMEvalKitæºç åšäº†å®šåˆ¶ï¼Œå‘½åä¸º`ms-vlmeval`ï¼Œè¯¥ç‰ˆæœ¬åœ¨åŸç‰ˆåŸºç¡€ä¸Šå¯¹è¯„ä¼°ä»»åŠ¡çš„é…ç½®å’Œæ‰§è¡Œè¿›è¡Œäº†å°è£…ï¼Œå¹¶æ”¯æŒpypiå®‰è£…æ–¹å¼ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡Eval-Scopeå‘èµ·è½»é‡åŒ–çš„VLMEvalKitè¯„ä¼°ä»»åŠ¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ”¯æŒåŸºäºOpenAI APIæ ¼å¼çš„æ¥å£è¯„ä¼°ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ModelScope [swift](https://github.com/modelscope/swift) éƒ¨ç½²å¤šæ¨¡æ€æ¨¡å‹æœåŠ¡ã€‚

##### å®‰è£…
```shell
# å®‰è£…é¢å¤–é€‰é¡¹
pip install llmuses[vlmeval]
```

##### æ•°æ®å‡†å¤‡
ç›®å‰æ”¯æŒçš„æ•°æ®é›†æœ‰ï¼š
```python
'COCO_VAL', 'MME', 'HallusionBench', 'POPE', 'MMBench_DEV_EN', 'MMBench_TEST_EN', 'MMBench_DEV_CN', 'MMBench_TEST_CN', 'MMBench', 'MMBench_CN', 'MMBench_DEV_EN_V11', 'MMBench_TEST_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_CN_V11', 'MMBench_V11', 'MMBench_CN_V11', 'SEEDBench_IMG', 'SEEDBench2', 'SEEDBench2_Plus', 'ScienceQA_VAL', 'ScienceQA_TEST', 'MMT-Bench_ALL_MI', 'MMT-Bench_ALL', 'MMT-Bench_VAL_MI', 'MMT-Bench_VAL', 'AesBench_VAL', 'AesBench_TEST', 'CCBench', 'AI2D_TEST', 'MMStar', 'RealWorldQA', 'MLLMGuard_DS', 'BLINK', 'OCRVQA_TEST', 'OCRVQA_TESTCORE', 'TextVQA_VAL', 'DocVQA_VAL', 'DocVQA_TEST', 'InfoVQA_VAL', 'InfoVQA_TEST', 'ChartQA_TEST', 'MathVision', 'MathVision_MINI', 'MMMU_DEV_VAL', 'MMMU_TEST', 'OCRBench', 'MathVista_MINI', 'LLaVABench', 'MMVet', 'MTVQA_TEST', 'MMLongBench_DOC', 'VCR_EN_EASY_500', 'VCR_EN_EASY_100', 'VCR_EN_EASY_ALL', 'VCR_EN_HARD_500', 'VCR_EN_HARD_100', 'VCR_EN_HARD_ALL', 'VCR_ZH_EASY_500', 'VCR_ZH_EASY_100', 'VCR_ZH_EASY_ALL', 'VCR_ZH_HARD_500', 'VCR_ZH_HARD_100', 'VCR_ZH_HARD_ALL', 'MMBench-Video', 'Video-MME', 'MMBench_DEV_EN', 'MMBench_TEST_EN', 'MMBench_DEV_CN', 'MMBench_TEST_CN', 'MMBench', 'MMBench_CN', 'MMBench_DEV_EN_V11', 'MMBench_TEST_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_CN_V11', 'MMBench_V11', 'MMBench_CN_V11', 'SEEDBench_IMG', 'SEEDBench2', 'SEEDBench2_Plus', 'ScienceQA_VAL', 'ScienceQA_TEST', 'MMT-Bench_ALL_MI', 'MMT-Bench_ALL', 'MMT-Bench_VAL_MI', 'MMT-Bench_VAL', 'AesBench_VAL', 'AesBench_TEST', 'CCBench', 'AI2D_TEST', 'MMStar', 'RealWorldQA', 'MLLMGuard_DS', 'BLINK'
```
æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚è€ƒ[VLMEvalKitæ”¯æŒçš„å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹é›†](https://github.com/open-compass/VLMEvalKit/blob/main/docs/zh-CN/README_zh-CN.md#%E6%94%AF%E6%8C%81%E7%9A%84%E5%9B%BE%E6%96%87%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%84%E6%B5%8B%E9%9B%86)
æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼Œæ¥æŸ¥çœ‹æ•°æ®é›†çš„åç§°åˆ—è¡¨ï¼š
```python
from llmuses.backend.vlm_eval_kit import VLMEvalKitBackendManager
print(f'** All models from VLMEvalKit backend: {VLMEvalKitBackendManager.list(list_supported_VLMs().keys())}')
```

åœ¨åŠ è½½æ•°æ®é›†æ—¶ï¼Œè‹¥æœ¬åœ°ä¸å­˜åœ¨è¯¥æ•°æ®é›†æ–‡ä»¶ï¼Œå°†ä¼šè‡ªåŠ¨ä¸‹è½½æ•°æ®é›†åˆ° `~/LMUData/` ç›®å½•ä¸‹ã€‚


##### æ¨¡å‹è¯„ä¼°
æ¨¡å‹è¯„ä¼°æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥é€‰æ‹©ï¼š

###### 1. ModelScope Swiftéƒ¨ç½²æ¨¡å‹æœåŠ¡è¯„ä¼°

**æ¨¡å‹éƒ¨ç½²**
ä½¿ç”¨ModelScope swiftéƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œå…·ä½“å¯å‚è€ƒï¼š[ModelScope Swift MLLM éƒ¨ç½²æŒ‡å—](https://swift.readthedocs.io/zh-cn/latest/Multi-Modal/MLLM%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3.html)
```shell
# å®‰è£…ms-swift
pip install ms-swift

# éƒ¨ç½²qwen-vl-chatå¤šæ¨¡æ€æ¨¡å‹æœåŠ¡
CUDA_VISIBLE_DEVICES=0 swift deploy --model_type qwen-vl-chat --model_id_or_path models/Qwen-VL-Chat
```

**æ¨¡å‹è¯„ä¼°**

å‚è€ƒç¤ºä¾‹æ–‡ä»¶ï¼š [example_eval_vlm_swift](examples/example_eval_vlm_swift.py) æ¥é…ç½®è¯„ä¼°ä»»åŠ¡
æ‰§è¡Œè¯„ä¼°ä»»åŠ¡ï¼š
```shell
python examples/example_eval_vlm_swift.py
```

###### 2. æœ¬åœ°æ¨¡å‹æ¨ç†è¯„ä¼°

**æ¨¡å‹æ¨ç†è¯„ä¼°**
ä¸å¯åŠ¨æ¨¡å‹æœåŠ¡ï¼Œç›´æ¥åœ¨æœ¬åœ°è¿›è¡Œæ¨ç†ï¼Œå‚è€ƒç¤ºä¾‹æ–‡ä»¶ï¼š [example_eval_vlm_local](examples/example_eval_vlm_local.py) æ¥é…ç½®è¯„ä¼°ä»»åŠ¡
æ‰§è¡Œè¯„ä¼°ä»»åŠ¡ï¼š
```shell
python examples/example_eval_vlm_local.py
```


##### (å¯é€‰) éƒ¨ç½²è£åˆ¤å‘˜æ¨¡å‹
éƒ¨ç½²æœ¬åœ°è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤ / é€‰æ‹©æå–å™¨ï¼ŒåŒæ ·ä½¿ç”¨ModelScope swiftéƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œå…·ä½“å¯å‚è€ƒï¼š[ModelScope Swift LLM éƒ¨ç½²æŒ‡å—](https://swift.readthedocs.io/zh-cn/latest/LLM/VLLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E4%B8%8E%E9%83%A8%E7%BD%B2.html)
ã€‚åœ¨æœªéƒ¨ç½²è£åˆ¤å‘˜æ¨¡å‹æ¨¡å‹æ—¶ï¼Œå°†ä½¿ç”¨ç²¾ç¡®åŒ¹é…ã€‚
```shell
# éƒ¨ç½²qwen2-7bä½œä¸ºè£åˆ¤å‘˜
CUDA_VISIBLE_DEVICES=1 swift deploy --model_type qwen2-7b-instruct --model_id_or_path models/Qwen2-7B-Instruct --port 8866
```
**å¿…é¡»é…ç½®è£åˆ¤å‘˜æ¨¡å‹ç¯å¢ƒå˜é‡æ‰èƒ½æ­£ç¡®è°ƒç”¨æ¨¡å‹**ï¼Œéœ€è¦é…ç½®çš„ç¯å¢ƒå˜é‡å¦‚ä¸‹ï¼š
```
OPENAI_API_KEY=EMPTY
OPENAI_API_BASE=http://127.0.0.1:8866/v1/chat/completions # è£åˆ¤å‘˜æ¨¡å‹çš„api_base
LOCAL_LLM=qwen2-7b-instruct #è£åˆ¤å‘˜æ¨¡å‹çš„ model_id
```


### ä½¿ç”¨æœ¬åœ°æ•°æ®é›†
æ•°æ®é›†é»˜è®¤æ‰˜ç®¡åœ¨[ModelScope](https://modelscope.cn/datasets)ä¸Šï¼ŒåŠ è½½éœ€è¦è”ç½‘ã€‚å¦‚æœæ˜¯æ— ç½‘ç»œç¯å¢ƒï¼Œå¯ä»¥ä½¿ç”¨æœ¬åœ°æ•°æ®é›†ï¼Œæµç¨‹å¦‚ä¸‹ï¼š
#### 1. ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°
```shell
# å‡å¦‚å½“å‰æœ¬åœ°å·¥ä½œè·¯å¾„ä¸º /path/to/workdir
wget https://modelscope.oss-cn-beijing.aliyuncs.com/open_data/benchmark/data.zip
unzip data.zip
```
åˆ™è§£å‹åçš„æ•°æ®é›†è·¯å¾„ä¸ºï¼š/path/to/workdir/data ç›®å½•ä¸‹ï¼Œè¯¥ç›®å½•åœ¨åç»­æ­¥éª¤å°†ä¼šä½œä¸º--dataset-dirå‚æ•°çš„å€¼ä¼ å…¥

#### 2. ä½¿ç”¨æœ¬åœ°æ•°æ®é›†åˆ›å»ºè¯„ä¼°ä»»åŠ¡
```shell
python llmuses/run.py --model ZhipuAI/chatglm3-6b --template-type chatglm3 --datasets arc --dataset-hub Local --dataset-args '{"arc": {"local_path": "/path/to/workdir/data/arc"}}' --limit 10

# å‚æ•°è¯´æ˜
# --dataset-hub: æ•°æ®é›†æ¥æºï¼Œæšä¸¾å€¼ï¼š `ModelScope`, `Local`, `HuggingFace` (TO-DO)  é»˜è®¤ä¸º`ModelScope`
# --dataset-dir: å½“--dataset-hubä¸º`Local`æ—¶ï¼Œè¯¥å‚æ•°æŒ‡æœ¬åœ°æ•°æ®é›†è·¯å¾„; å¦‚æœ--dataset-hub è®¾ç½®ä¸º`ModelScope` or `HuggingFace`ï¼Œåˆ™è¯¥å‚æ•°çš„å«ä¹‰æ˜¯æ•°æ®é›†ç¼“å­˜è·¯å¾„ã€‚
```

#### 3. (å¯é€‰)åœ¨ç¦»çº¿ç¯å¢ƒåŠ è½½æ¨¡å‹å’Œè¯„æµ‹
æ¨¡å‹æ–‡ä»¶æ‰˜ç®¡åœ¨ModelScope Hubç«¯ï¼Œéœ€è¦è”ç½‘åŠ è½½ï¼Œå½“éœ€è¦åœ¨ç¦»çº¿ç¯å¢ƒåˆ›å»ºè¯„ä¼°ä»»åŠ¡æ—¶ï¼Œå¯å‚è€ƒä»¥ä¸‹æ­¥éª¤ï¼š
```shell
# 1. å‡†å¤‡æ¨¡å‹æœ¬åœ°æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å¤¹ç»“æ„å‚è€ƒchatglm3-6bï¼Œé“¾æ¥ï¼šhttps://modelscope.cn/models/ZhipuAI/chatglm3-6b/files
# ä¾‹å¦‚ï¼Œå°†æ¨¡å‹æ–‡ä»¶å¤¹æ•´ä½“ä¸‹è½½åˆ°æœ¬åœ°è·¯å¾„ /path/to/ZhipuAI/chatglm3-6b

# 2. æ‰§è¡Œç¦»çº¿è¯„ä¼°ä»»åŠ¡
python llmuses/run.py --model /path/to/ZhipuAI/chatglm3-6b --template-type chatglm3 --datasets arc --dataset-hub Local --dataset-args '{"arc": {"local_path": "/path/to/workdir/data/arc"}}' --limit 10
```


### ä½¿ç”¨run_taskå‡½æ•°æäº¤è¯„ä¼°ä»»åŠ¡

#### 1. é…ç½®ä»»åŠ¡
```python
import torch
from llmuses.constants import DEFAULT_ROOT_CACHE_DIR

# ç¤ºä¾‹
your_task_cfg = {
        'model_args': {'revision': None, 'precision': torch.float16, 'device_map': 'auto'},
        'generation_config': {'do_sample': False, 'repetition_penalty': 1.0, 'max_new_tokens': 512},
        'dataset_args': {},
        'dry_run': False,
        'model': 'ZhipuAI/chatglm3-6b',
        'template_type': 'chatglm3', 
        'datasets': ['arc', 'hellaswag'],
        'work_dir': DEFAULT_ROOT_CACHE_DIR,
        'outputs': DEFAULT_ROOT_CACHE_DIR,
        'mem_cache': False,
        'dataset_hub': 'ModelScope',
        'dataset_dir': DEFAULT_ROOT_CACHE_DIR,
        'stage': 'all',
        'limit': 10,
        'debug': False
    }

```

#### 2. æ‰§è¡Œä»»åŠ¡
```python
from llmuses.run import run_task

run_task(task_cfg=your_task_cfg)
```


### ç«æŠ€åœºæ¨¡å¼ï¼ˆArenaï¼‰
ç«æŠ€åœºæ¨¡å¼å…è®¸å¤šä¸ªå€™é€‰æ¨¡å‹é€šè¿‡ä¸¤ä¸¤å¯¹æ¯”(pairwise battle)çš„æ–¹å¼è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å¯ä»¥é€‰æ‹©å€ŸåŠ©AI Enhanced Auto-Reviewerï¼ˆAARï¼‰è‡ªåŠ¨è¯„ä¼°æµç¨‹æˆ–è€…äººå·¥è¯„ä¼°çš„æ–¹å¼ï¼Œæœ€ç»ˆå¾—åˆ°è¯„ä¼°æŠ¥å‘Šï¼Œæµç¨‹ç¤ºä¾‹å¦‚ä¸‹ï¼š
#### 1. ç¯å¢ƒå‡†å¤‡
```text
a. æ•°æ®å‡†å¤‡ï¼Œquestions dataæ ¼å¼å‚è€ƒï¼šllmuses/registry/data/question.jsonl
b. å¦‚æœéœ€è¦ä½¿ç”¨è‡ªåŠ¨è¯„ä¼°æµç¨‹ï¼ˆAARï¼‰ï¼Œåˆ™éœ€è¦é…ç½®ç›¸å…³ç¯å¢ƒå˜é‡ï¼Œæˆ‘ä»¬ä»¥GPT-4 based auto-revieweræµç¨‹ä¸ºä¾‹ï¼Œéœ€è¦é…ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
> export OPENAI_API_KEY=YOUR_OPENAI_API_KEY
```

#### 2. é…ç½®æ–‡ä»¶
```text
arenaè¯„ä¼°æµç¨‹çš„é…ç½®æ–‡ä»¶å‚è€ƒï¼š llmuses/registry/config/cfg_arena.yaml
å­—æ®µè¯´æ˜ï¼š
    questions_file: question dataçš„è·¯å¾„
    answers_gen: å€™é€‰æ¨¡å‹é¢„æµ‹ç»“æœç”Ÿæˆï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹ï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ¨¡å‹
    reviews_gen: è¯„ä¼°ç»“æœç”Ÿæˆï¼Œç›®å‰é»˜è®¤ä½¿ç”¨GPT-4ä½œä¸ºAuto-reviewerï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ­¥éª¤
    elo_rating: ELO rating ç®—æ³•ï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ­¥éª¤ï¼Œæ³¨æ„è¯¥æ­¥éª¤ä¾èµ–review_fileå¿…é¡»å­˜åœ¨
```

#### 3. æ‰§è¡Œè„šæœ¬
```shell
#Usage:
cd llmuses

# dry-runæ¨¡å¼ (æ¨¡å‹answeræ­£å¸¸ç”Ÿæˆï¼Œä½†ä¸“å®¶æ¨¡å‹ï¼Œå¦‚GPT-4ï¼Œä¸ä¼šè¢«è°ƒç”¨ï¼Œè¯„ä¼°ç»“æœä¼šéšæœºç”Ÿæˆ)
python llmuses/run_arena.py -c registry/config/cfg_arena.yaml --dry-run

# æ‰§è¡Œè¯„ä¼°æµç¨‹
python llmuses/run_arena.py --c registry/config/cfg_arena.yaml
```

#### 4. ç»“æœå¯è§†åŒ–

```shell
# Usage:
streamlit run viz.py -- --review-file llmuses/registry/data/qa_browser/battle.jsonl --category-file llmuses/registry/data/qa_browser/category_mapping.yaml
```


### å•æ¨¡å‹æ‰“åˆ†æ¨¡å¼ï¼ˆSingle modeï¼‰

è¿™ä¸ªæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬åªå¯¹å•ä¸ªæ¨¡å‹è¾“å‡ºåšæ‰“åˆ†ï¼Œä¸åšä¸¤ä¸¤å¯¹æ¯”ã€‚
#### 1. é…ç½®æ–‡ä»¶
```text
è¯„ä¼°æµç¨‹çš„é…ç½®æ–‡ä»¶å‚è€ƒï¼š llmuses/registry/config/cfg_single.yaml
å­—æ®µè¯´æ˜ï¼š
    questions_file: question dataçš„è·¯å¾„
    answers_gen: å€™é€‰æ¨¡å‹é¢„æµ‹ç»“æœç”Ÿæˆï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹ï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ¨¡å‹
    reviews_gen: è¯„ä¼°ç»“æœç”Ÿæˆï¼Œç›®å‰é»˜è®¤ä½¿ç”¨GPT-4ä½œä¸ºAuto-reviewerï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ­¥éª¤
    rating_gen: rating ç®—æ³•ï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ­¥éª¤ï¼Œæ³¨æ„è¯¥æ­¥éª¤ä¾èµ–review_fileå¿…é¡»å­˜åœ¨
```
#### 2. æ‰§è¡Œè„šæœ¬
```shell
#Example:
python llmuses/run_arena.py --c registry/config/cfg_single.yaml
```

### Baselineæ¨¡å‹å¯¹æ¯”æ¨¡å¼ï¼ˆPairwise-baseline modeï¼‰

è¿™ä¸ªæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬é€‰å®š baseline æ¨¡å‹ï¼Œå…¶ä»–æ¨¡å‹ä¸ baseline æ¨¡å‹åšå¯¹æ¯”è¯„åˆ†ã€‚è¿™ä¸ªæ¨¡å¼å¯ä»¥æ–¹ä¾¿çš„æŠŠæ–°æ¨¡å‹åŠ å…¥åˆ° Leaderboard ä¸­ï¼ˆåªéœ€è¦å¯¹æ–°æ¨¡å‹è·Ÿ baseline æ¨¡å‹è·‘ä¸€éæ‰“åˆ†å³å¯ï¼‰
#### 1. é…ç½®æ–‡ä»¶
```text
è¯„ä¼°æµç¨‹çš„é…ç½®æ–‡ä»¶å‚è€ƒï¼š llmuses/registry/config/cfg_pairwise_baseline.yaml
å­—æ®µè¯´æ˜ï¼š
    questions_file: question dataçš„è·¯å¾„
    answers_gen: å€™é€‰æ¨¡å‹é¢„æµ‹ç»“æœç”Ÿæˆï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹ï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ¨¡å‹
    reviews_gen: è¯„ä¼°ç»“æœç”Ÿæˆï¼Œç›®å‰é»˜è®¤ä½¿ç”¨GPT-4ä½œä¸ºAuto-reviewerï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ­¥éª¤
    rating_gen: rating ç®—æ³•ï¼Œå¯é€šè¿‡enableå‚æ•°æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥æ­¥éª¤ï¼Œæ³¨æ„è¯¥æ­¥éª¤ä¾èµ–review_fileå¿…é¡»å­˜åœ¨
```
#### 2. æ‰§è¡Œè„šæœ¬
```shell
# Example:
python llmuses/run_arena.py --c registry/config/cfg_pairwise_baseline.yaml
```


## æ•°æ®é›†åˆ—è¡¨

| DatasetName        | Link                                                                                   | Status | Note |
|--------------------|----------------------------------------------------------------------------------------|--------|------|
| `mmlu`             | [mmlu](https://modelscope.cn/datasets/modelscope/mmlu/summary)                         | Active |      |
| `ceval`            | [ceval](https://modelscope.cn/datasets/modelscope/ceval-exam/summary)                  | Active |      |
| `gsm8k`            | [gsm8k](https://modelscope.cn/datasets/modelscope/gsm8k/summary)                       | Active |      |
| `arc`              | [arc](https://modelscope.cn/datasets/modelscope/ai2_arc/summary)                       | Active |      |
| `hellaswag`        | [hellaswag](https://modelscope.cn/datasets/modelscope/hellaswag/summary)               | Active |      |
| `truthful_qa`      | [truthful_qa](https://modelscope.cn/datasets/modelscope/truthful_qa/summary)           | Active |      |
| `competition_math` | [competition_math](https://modelscope.cn/datasets/modelscope/competition_math/summary) | Active |      |
| `humaneval`        | [humaneval](https://modelscope.cn/datasets/modelscope/humaneval/summary)               | Active |      |
| `bbh`              | [bbh](https://modelscope.cn/datasets/modelscope/bbh/summary)                           | Active |      |
| `race`             | [race](https://modelscope.cn/datasets/modelscope/race/summary)                         | Active |      |
| `trivia_qa`        | [trivia_qa](https://modelscope.cn/datasets/modelscope/trivia_qa/summary)               | To be intergrated |      |


## Leaderboard æ¦œå•
ModelScope LLM Leaderboardå¤§æ¨¡å‹è¯„æµ‹æ¦œå•æ—¨åœ¨æä¾›ä¸€ä¸ªå®¢è§‚ã€å…¨é¢çš„è¯„ä¼°æ ‡å‡†å’Œå¹³å°ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…äº†è§£å’Œæ¯”è¾ƒModelScopeä¸Šçš„æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚

[Leaderboard](https://modelscope.cn/leaderboard/58/ranking?type=free)



## å®éªŒå’ŒæŠ¥å‘Š
å‚è€ƒï¼š [Experiments](./resources/experiments.md)

## æ€§èƒ½è¯„æµ‹å·¥å…·
å‚è€ƒï¼š [æ€§èƒ½æµ‹è¯•](llmuses/perf/README.md)

## TO-DO List
- [ ] Agents evaluation
- [ ] vLLM
- [ ] Distributed evaluating
- [ ] Multi-modal evaluation
- [ ] Benchmarks
  - [ ] GAIA
  - [ ] GPQA
  - [ ] MBPP
- [ ] Auto-reviewer
  - [ ] Qwen-max


